{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "-woU4Sodh6ND",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Assignment #2\n",
    "\n",
    "\n",
    "Deep Learning / Spring 1398, Iran University of Science and Technology\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "JWitIy1viFuD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "- **Assignment Due: ** 1398/01/10 23:59\n",
    "- If you need any additional information, please review the assignment page on the course website.\n",
    "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
    "```\n",
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################\n",
    "```\n",
    "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
    "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
    "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
    "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
    "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of its depencecies.\n",
    "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "Assignment Page: [https://iust-deep-learning.github.io/972/assignments/02_tuning](https://iust-deep-learning.github.io/972/assignments/02_tuning)\n",
    "\n",
    "Course Forum: [https://groups.google.com/forum/#!forum/dl972/](https://groups.google.com/forum/#!forum/dl972/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "0ejALNiDCWnd",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "Vjwf6dWNCIQP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Fill your information here & run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "NeLkOPE6Qwr7",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
    "student_id = 0 #@param {type:\"integer\"}\n",
    "student_name = \"\" #@param {type:\"string\"}\n",
    "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
    "\n",
    "print(\"your student id:\", student_id)\n",
    "print(\"your name:\", student_name)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ASSIGNMENT_PATH = Path('asg02')\n",
    "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "QfSFVOaKvlDb",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 1. PlayOfTheGame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "UmprL4aETrj8",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 1.1 What is PUBG?\n",
    "---\n",
    "PlayerUnknown's Battlegrounds (PUBG) is a popular online survival multiplayer game. In this game, players are dropped into a wide, open area, and they must fight to the death using a variety of interesting weapons and vehicles while avoiding getting killed themselves. The last player or team standing wins the round. Although it's not necessary, but you can learn about other aspects of the game easily by searching the web, since the game is very popular and well known.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "NLU26ncrYZL5",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 1.2 Kaggle competition - Can you predict the battle royale finish of PUBG Players?\n",
    "---\n",
    "Kaggle is a platform to compete with others in competitions which are based on machine learning tasks. Most of the time you are given some training and testing datasets for a specific task to build some good machine learning models.\n",
    "\n",
    "In this assignment, you will participate in one of these competitions which is realted to PUBG. See [this](https://www.kaggle.com/c/pubg-finish-placement-prediction) link for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "x6mFFJsP6vRC",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 1.3 Data exploration and feature selection\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "bAbR3KNYN9Yw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's download the sampled dataset (100K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "iRVN68kaOFXX",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "! wget -q https://iust-deep-learning.github.io/972/static_files/assignments/asg02_assets/data.tar.gz\n",
    "! tar xvfz data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "kr2IFQDgOlDD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then, load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "6aY8nBsw23Qw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('valid.csv')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "tFqWPWvot29x",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As you can see, the training dataset consists of lots of different features for each instance, choose some arbitary numer of these features (at least three) which you think they are better for training. Explain how did you find them and why do you think this way?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "Ob0fG9c9ws5w",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write your answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "W0bJXAiH7Q6-",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 1.4 Implementation\n",
    "\n",
    "---\n",
    "\n",
    "Build and train a simple feed forward neural network regressor using your selected features to predict the desired outcome (player's final percentile rank). Choosing the number of layers, activasion functions, the optimizer, representation of input data and hyper parameters are completely up to you. Also feel free to add any new cells, functions, and classes if you want.\n",
    "\n",
    "\n",
    "Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "PNVZRXzM7Gnk",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "7vtlQwS87p59",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "3ZxGPuuF7tM1",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Go on and use whatever MLP architecture you want\n",
    "# Layers, and number of them is Totally up to you\n",
    "\n",
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "69-hgID38Q8E",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "HLkkR6I28UOJ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Remember to run this cell after each time you update the model, \n",
    "# this is one of deliverable items of your assignemnt\n",
    "model.save(str(ASSIGNMENT_PATH / 'potg.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "91YQbfvV8s7l",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 1.5 Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "In order to evaluate your model, we need you to fill the following function. Remember, all features are present in the input file, so you must choose your selected features, do all the requiered pre processing, feed your trained model with the result and finaly give us your predictions in a list.\n",
    "\n",
    "**Note:** We'll run your model on a hidden test set using this function to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "LBco1Zh89y10",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "  \"\"\"\n",
    "  Predict the placement of a player.\n",
    "  \n",
    "  Args:\n",
    "    x (list[tuple()]): A list of players. Each player is a tuple(Id, groupId, matchId,\n",
    "      assists, boosts, damageDealt, DBNOs, headshotKills, heals, killPlace, killPoints,\n",
    "      kills, killStreaks, longestKill, matchDuration, matchType, maxPlace, numGroups,\n",
    "      rankPoints, revives, rideDistance, roadKills, swimDistance, teamKills, vehicleDestroys,\n",
    "      walkDistance, weaponsAcquired, winPoints)\n",
    "  \n",
    "  Returns:\n",
    "    pred (list[float]): contains the placement prediction for each element in the input list.\n",
    "      predictions are of 0-1 range where 1 corresponds to 1st place, and 0 corresponds\n",
    "      to last place in the match\n",
    "  \"\"\"\n",
    "  \n",
    "  m = load_model(str(ASSIGNMENT_PATH / 'potg.h5'))\n",
    "  pred = []\n",
    "  \n",
    "  # Do all of the preprocessing here,\n",
    "  # you can use any combination of features you want.\n",
    "  \n",
    "  ########################################\n",
    "  #     Put your implementation here     #\n",
    "  ########################################\n",
    "  \n",
    "  assert isinstance(pred, list)\n",
    "  assert len(pred) == len(x)\n",
    "  assert all([isinstance(p, float) for p in pred])\n",
    "  \n",
    "  return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "aSpe-y2fj8Mp",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Notes\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "*   The original train  dataset has about 5 million records which is too large, but you can use our 100k sampled version which is provided for you and has almost the same distribuition as the original one.\n",
    "*  Since you are using a simple feedforward NN, your results don't have to be extraordinary! You will be graded in a comparative manner, so just try your best.\n",
    "*   There are lots of shared codes and ideas from other competitors for this challenge [here](https://www.kaggle.com/c/pubg-finish-placement-prediction/kernels). Feel free to take a look at these shared information, you can even get ideas and try them yourself, but be sure to not copy anything as copying has very serious consequences! You might even find similar implementations to what you have to do, but remember if you are able to find those implementations, so are we ;) .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "DAxg5j543lc5",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 2. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "jsfUGOwYyRSm",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 2.1 Underfitting and Overfitting, how to deal with them?\n",
    "---\n",
    "By using a neural network, we try to approximate a function for different purposes.  In the training process, we want to maximize accuracy, while minimizing the error rate. However, there might be some problems with the model that we train.\n",
    "\n",
    "One of the problems with deep neural networks is that they perform poorly in some cases. This poor performance might have different reasons. As you see in the pictures below, the problem might be due to the function we use, which might be too simple for such a task (underfitting) or too complex (overfitting).\n",
    "\n",
    "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/1__7OPgojau8hkiPUiHoGK_w.png\"  width=500 />\n",
    "\n",
    "[(source)](https://algotrading101.com/blog/15001/what-is-curve-fitting-overfitting-in-trading-optimization)\n",
    "\n",
    "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/1_JZbxrdzabrT33Yl-LrmShw.png\"  width=500 />\n",
    "\n",
    "[(source)](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)\n",
    "\n",
    "In the above pictures, the left graphs demonstrate an **underfitted function** that performs poorly on the task. This function has a high bias. On the other hand, the right graphs have a low bias and high variance. These are too-compilicated functions that have unnecessarily learned noisy details of the training set. To better understand this concept, let us explain what we mean by bias and variance.\n",
    "\n",
    "**Bias** is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to high error on training and test data.\n",
    "\n",
    "\n",
    "**Variance** is the variability of model prediction for a given data point or a value which reflects the spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data.\n",
    "\n",
    "In our models, we should try to balance the tradeoff between bias and variance so that the model can perform well on the test set. Overfitting is a more common problem in the training process and one way of recognizing it is by looking at the learning curves. In the below graph, the red curve is the validation error and the blue curve is the training error per each epoch of learning. The indication for the start of overfitting on the training set is that training error start to decline whereas the validation error increases.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/Overfitting_svg.svg/450px-Overfitting_svg.svg.png\" width=300 />\n",
    "\n",
    "[(source)](https://en.wikipedia.org/wiki/Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "X2xWtaIpzWAH",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 2.2 How to overcome overfitting?\n",
    "\n",
    "---\n",
    "One way to overcome overfitting is through regularization. There are different regularization methods such as L2 or L1 regularization. In regularization, we add an extra term to the loss function of the neural network. This extra term could be L2 norm of weight matrices or their L1 norm. So, the cost function will be similar the following equation:\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost function = Loss + \\frac{\\lambda}{ 2m} \\sum_{i} \\sum_{j} \\left \\lvert\\lvert w_{i, j} \\right \\rvert\\rvert^{2}_{F}\n",
    "\\end{equation*}\n",
    "\n",
    "Instead of L2 norm, we can use L1 norm or a linear combination of each one. We can compute L1 norm and a linear combination of L1 and L2 norm using the following equations:\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost function = Loss + \\frac{\\lambda}{ 2m} \\sum_{i} \\sum_{j} \\left \\lvert\\lvert w_{i, j} \\right \\rvert\\rvert\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Cost function = Loss + \\alpha( \\frac{\\lambda_{l2}}{ 2m} \\sum_{i} \\sum_{j} \\left \\lvert\\lvert w_{i, j} \\right \\rvert\\rvert^{2}_{F} ) + (1 - \\alpha) (\\frac{\\lambda_{l1}}{ 2m} \\sum_{i} \\sum_{j} \\left \\lvert\\lvert w_{i, j} \\right \\rvert\\rvert ) \n",
    "\\end{equation*}\n",
    "\n",
    "And $\\alpha$ could be any real number between 0 and 1. \n",
    "\n",
    "Adding the L1 norm to the cost function forces the weights to be close to zero, and this will lead to sparse weight matrices. This sparsity helps us to overcome the overfitting problem, because it limits the domain of possible values for weights and this prevents the function to be very compilicated. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "jPo4jqofzcoF",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### 2.3 How to use regularization methods in Keras?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In this assignment, we want to learn how to use regularization techniques in Keras and how they will affect weight matrices. In Keras, we can use regularizations for weight matrices, biases, and activations. To use regularization techniques, you should, for each layer, specify weather you want to use L1, L2, or a combination of them.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from keras import regularizers\n",
    "\n",
    "Dense(64, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01))\n",
    "```\n",
    "\n",
    "As you see in the above code, we can easily use the regularization techniques in each of the layers. You can set regularizer for weight matrix, bias vector, and activations of a layer by using the ```kernel_regularizer, bias_regularizer, activity_regularizer``` parameters, respectively. As you see, we used an L2 norm with $\\lambda = 0.01$ to penalize the weight matrix, an L1 norm to penalize the bias vector, and a combination of L1 and L2 norms to penalize activations.\n",
    "\n",
    "In Keras, we can also use other custom regularization approaches (which may not have been implemented in the framework). To implement and use a new regularization method, we should define a method like the following code and then pass it to the layer. In the following code, we implemented the L1 norm.\n",
    "\n",
    "\n",
    "```python\n",
    "from keras import backend as K\n",
    "\n",
    "def l1_reg(weight_matrix):\n",
    "    return 0.01 * K.sum(K.abs(weight_matrix))\n",
    "\n",
    "model.add(Dense(64, input_dim=64, kernel_regularizer=l1_reg))\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "eJY8nM7zzk8u",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Questions**\n",
    "\n",
    "We would like to train a neural network that learns to classify the data that in the following graph.\n",
    "\n",
    "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/1.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "vr4CViCkz_AT",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data(nb_samples_per_class):\n",
    "  mean = [0, 0]\n",
    "  cov = [[.01, 0], [.014, 0.05]]\n",
    "  x, y = np.random.multivariate_normal(mean, cov, nb_samples_per_class).T\n",
    "\n",
    "  mean = [.4, .1]\n",
    "  cov = [[0.01, .01], [.04, .01]]\n",
    "  x1, y1 = np.random.multivariate_normal(mean, cov, nb_samples_per_class).T\n",
    "\n",
    "  d1 = [[i, j, 1] for i, j in zip(x, y)]\n",
    "  d2 = [[i, j, 0] for i, j in zip(x1, y1)]\n",
    "  data = np.array(d1 + d2)\n",
    "  np.random.shuffle(data)\n",
    "\n",
    "  return data\n",
    "  \n",
    "  \n",
    "data = get_data(100)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=data[:, 2].ravel(), cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "Xynf9A8U0Wsv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Training**\n",
    "\n",
    "We trained the following model, and then, plotted the decision boundaries.\n",
    "Decision boundaries have been presented in the following graph.\n",
    "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/2.png\" width=\"700\" />\n",
    "\n",
    "As you see, the approximated function is a very complex one that concentrates on the training set and cannot generalize well on the test test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "RvzzCVv60nZw",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# For a single-input model with 2 classes (binary classification):\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "val_data = get_data(30)\n",
    "callback = model.fit(data[:, :2], data[:, 2], validation_data=(val_data[:, :2], val_data[:, 2]), epochs=2000, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "aRY4b3t71AKv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "  y = np.around(y)\n",
    "  # Set min and max values and give it some padding\n",
    "  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "  h = .01\n",
    "  # Generate a grid of points with distance h between them\n",
    "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "  # Predict the function value for the whole grid\n",
    "  Z = np.around(model(np.c_[xx.ravel(), yy.ravel()]))\n",
    "  Z = Z.reshape(xx.shape)\n",
    "\n",
    "  # Plot the contour and training examples\n",
    "  plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "  plt.ylabel('x2')\n",
    "  plt.xlabel('x1')\n",
    "  plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap=plt.cm.Spectral)\n",
    "  plt.show()\n",
    "    \n",
    "label = model.predict(data[:, :2])\n",
    "plot_decision_boundary(lambda x : model.predict(x), data[:, :2], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "pt5oZm071Tlk",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "`model.fit()` method returns a callback that contains history of the learning process. You can access the loss and accuracy of the model on training and validation sets in each epoch of learning.\n",
    "\n",
    "```python\n",
    "callback.history['acc']\n",
    "callback.history['loss']\n",
    "callback.history['val_acc']\n",
    "callback.history['val_loss']\n",
    "```\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "**Note:** In the following questions, whenever we mentioned learning curves, we mean three graphs. These three graphs indicate accuracy of model per epoch, error rate of the model per epoch, and the value of network loss for training, validation, and test sets.\n",
    "\n",
    "**Note:** You can use ```plot_decision_boundary``` method to plot the decision boundaries.\n",
    "\n",
    "**Note:** If learning curves oscillate, you can use moving average to smooth them. Use the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "1hiE4gEv1ddE",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def moving_avg(mist):\n",
    "  N = 30\n",
    "  cumsum, moving_aves = [0], []\n",
    "\n",
    "  for i, x in enumerate(mlist, 1):\n",
    "    cumsum.append(cumsum[i - 1] + x)\n",
    "    if i >= N:\n",
    "      moving_ave = (cumsum[i] - cumsum[i - N]) / N\n",
    "      # can do stuff with moving_ave here\n",
    "      moving_aves.append(moving_ave)\n",
    "  return moving_aves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "my4WuXgzvXLV",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**1)**   Plot learning curves and point out the approximate epoch that the model started to overfit on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "u0Hy9pI_uFep",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "939KiRYSuDRk",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**2)** Apply L1 and L2, separately, on all of the layers (just for weight matrices) and plot the learning curves and decision boundaries. Test it with three different values for $\\lambda$ ($\\lambda \\in \\{0.1, 0.01, 0.0001\\} $). Which values work better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "w5otBrfWubQX",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "zolZM-3W30aN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "kZ3d-1J2uNrX",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**3)** Now, apply the L1 and L2 on biases and compare with the result of last question (compare each $\\lambda$ separately). Which one works better? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "9Hav3NKLucGT",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "v6f30yjLfHCq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "pH914LqYuNg_",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**4)** Implement a linear combination of L1 and L2 norm and test it for three different value of $\\alpha$ ($\\alpha \\in \\{0.3, 0.5, 0.7\\} $). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "iNgKJvpnuc0J",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "ag8HErb-uNVL",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**5)** Compare the results of questions 2 and 4 for each value of $\\alpha$ separately (one plot for each value of $\\alpha$ that contains learning curves for L1, L2, and linear combination of them). $\\lambda = 0.01$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "1-ZMGHsDfKyN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "6BR90mxVuVC3",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**6)**  Try to prevent the overfitting by adding the regularization term to each layer of the network, and then, plot the decision boundaries and learning curves. Add each of the regularization techniques seperatly and compare them with eachother. $\\lambda = 0.01$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "LDjji3tTueqR",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "O-SH5FDsuZjG",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**7)** Run you implemented code for question 2 with $\\lambda = 0.01$ multiple times. Which regularization technique is stable? (By stable, we mean a model that prevents the overfitting all the time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "eeX9QL1uuhRx",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "#     Put your implementation here     #\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "YaW3DXIt4Tqj",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "zeDLckyY_Yuq",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 3. Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "Xl8ZYWYb6CD-",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Run the following block to import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "Nh0E0W84xALN",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "hAWJRALvZJHj",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part 1: Minimizing a custom function\n",
    "Consider this structure: a smiple model with a single node in input layer and a single node in output dense layer (with **`use_bias`**  argument set to **`False`**). This way, if we set the input to the constant value 1, the output will always be equal to the single weight variable between the input node and the output node.\n",
    "Using this technique, we can define a custom arbitary function and find its minimum value using predefined optimizer methods in keras.\n",
    "\n",
    "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/opt_01.jpg\" width=\"500\" />\n",
    "\n",
    "See the following code for better underestanding:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "yfc6x2UbBcn4",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def minimize (config):\n",
    "\n",
    "  '''\n",
    "  The wrapper function which makes\n",
    "  the custom fucntion suitable for\n",
    "  model.compile method\n",
    "  '''\n",
    "  def custom_loss(layer):\n",
    "    \n",
    "    '''\n",
    "    custom function f(x) = x**2\n",
    "    Notice that y_pred value is exactly\n",
    "    equal to our single weight value as\n",
    "    explained before. Also Notice that \n",
    "    y_ture value dosn't actually play\n",
    "    any rule in defined function, but \n",
    "    it needs to be passed.\n",
    "    '''\n",
    "    def loss(y_true,y_pred):\n",
    "      # in order to change objective function, this line must be changed\n",
    "      return ((y_pred-1)**2)*(y_pred+1)*(y_pred**2-3)*(y_pred-4)/90.0\n",
    "    \n",
    "    return loss\n",
    "\n",
    "  \n",
    "  # Creating single input single output model \n",
    "  init_vals = config['init_vals']\n",
    "  inp = Input(shape=(1,))\n",
    "  weights = Dense(1, use_bias=False) \n",
    "  out = weights(inp)\n",
    "  model = Model (inputs=inp, outputs=out)\n",
    "  weights.set_weights([np.array([init_vals])])\n",
    "  model.compile (optimizer=config['optimizer'], loss=custom_loss(out))\n",
    "  \n",
    "  \n",
    "  # Storing w1 (our single weight) values\n",
    "  # during training for later plotting. \n",
    "  w1_history = [init_vals[0]]\n",
    "  for epoch in range(config['epochs']):\n",
    "    # Notice the constant 1 input value.\n",
    "    # Also Notice that the output value\n",
    "    # passed to fit method dosn't really\n",
    "    # matter, however it can not be None\n",
    "    # and needs to be passed. \n",
    "    model.fit (x=[1.0], y=[1.0], epochs= 1, verbose=0);\n",
    "    w1_history.append (weights.get_weights()[0][0][0])\n",
    "    \n",
    "  return w1_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "1F7Pg4OaZh8J",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part 2: Visualising\n",
    "Using this piece of code, we can visualize the optimizer's steps for minimizing the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "0EnTr4FbZhYa",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize(independent_variable_history):\n",
    "  fig = plt.figure(figsize = (4,4))\n",
    "  \n",
    "  X = np.linspace(-2.1, 4.1, 200)\n",
    "  \n",
    "  '''\n",
    "  @@ in order to change objective function, this line\n",
    "  must be changed\n",
    "  '''\n",
    "  Y = ((X-1)**2)*(X+1)*(X**2-3)*(X-4)/90.0\n",
    "  \n",
    "  def ani(coords):\n",
    "    plt.cla()\n",
    "    plt.plot(X, Y, \"b\")\n",
    "    return plt.plot([coords[0]],[coords[1]], 'go')\n",
    "\n",
    "  def frames():\n",
    "      for x in independent_variable_history:\n",
    "          '''\n",
    "          @@ in order to change objective function, this line\n",
    "          must be changed\n",
    "          '''\n",
    "          yield x, ((x-1)**2)*(x+1)*(x**2-3)*(x-4)/90.0\n",
    "\n",
    "  from IPython.display import HTML\n",
    "  return HTML(animation.FuncAnimation(fig, ani, frames=frames, interval=30).to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "J-81ynhYZP7p",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can use these codes by passing a configuration dictionary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "t_wEYuKKnfhQ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"init_vals\": [-2.0],\n",
    "    \"optimizer\": optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9),\n",
    "    \"epochs\" : 200\n",
    "}\n",
    "independent_variable_history = minimize(config)\n",
    "visualize(independent_variable_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "hrYg8kb2Lus6",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Part 3: Your part\n",
    "\n",
    "\n",
    "*   **A)**  In the previous proposed technique, we had only a single independent variable (the single weight of the model). Expand this idea in a way that supports more than one independent variable, explain your toughts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "NNZZRyWgNKSZ",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "B3Wk6L85NO5m",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "*  **B)** Change the optimizer config values in order to make the model fall in the other local minimums.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "OfPXrfpNP2eD",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#### FIRST LOCAL MINIMUM\n",
    "\n",
    "config = {\n",
    "    \"init_vals\": [-2.0],\n",
    "    \"optimizer\": optimizers.SGD(lr= ? , decay= ? , momentum= ?),\n",
    "    \"epochs\" : 200\n",
    "}\n",
    "independent_variable_history = minimize(config)\n",
    "visualize(independent_variable_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "3ey2JNGiP2bh",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#### SECOND LOCAL MINIMUM\n",
    "\n",
    "config = {\n",
    "    \"init_vals\": [-2.0],\n",
    "    \"optimizer\": optimizers.SGD(lr= ? , decay= ? , momentum= ?),\n",
    "    \"epochs\" : 200\n",
    "}\n",
    "independent_variable_history = minimize(config)\n",
    "visualize(independent_variable_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "BhXlyRUJXeEW",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "*   **C)** Explain how does each of SGD configuration parameters affect the behaviour of optmizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "ZqBD4tKLY3Db",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "G-ZKK6SsQ3A2",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "*  **D)** Checkout Adam optimizer in keras documentation. Use Adam optimizer instead of SGD. Try different parameter configurations and see the effects. Based on your observations, explain how does each of these parameters affect the behaviour of Adam optimizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "jScpTJjnXS76",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "hPuqA5kYWeEA",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "*  **E)** In which situations do you think using Adam optimizer would be more effective than regular SGD optimizer? Explain your reasons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "9Cdj6vPGXUJ0",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "$\\color{red}{\\text{Write you answer here}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "pBjq-MvamPXO",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "7o4q5LiFiOx1",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
    "\n",
    "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
    "2. Select File > Save.\n",
    "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
    "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
    "5. Grab downloaded file (`dl_asg02__xx__xx.zip`) and submit it via https://goo.gl/forms/ShToQRtEp32n8YHq2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "iWRUf35av3ZP",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Note: ** We need your Github token to create (if doesn't exist previously) new repository to store learned model data. Also Google Drvie token enable us to download current notebook & create submission. If you are intrested feel free to check our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "cTytERc-vlaK",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Create Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "r4o37hc3AEUg",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "! pip install -U --quiet PyDrive > /dev/null\n",
    "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
    "  \n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "from google.colab import files\n",
    "from IPython.display import Javascript\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "asg_name = 'assignment_02'\n",
    "script_save = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "repo_name = 'iust-deep-learning-assignments'\n",
    "submission_file_name = 'dl_asg02__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
    "\n",
    "! tar xf hub-linux-amd64-2.10.0.tgz\n",
    "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
    "! hub config --global hub.protocol https\n",
    "! hub config --global user.email \"$Your_Github_account_Email\"\n",
    "! hub config --global user.name \"$student_name\"\n",
    "! hub api --flat -X GET /user\n",
    "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
    "\n",
    "repos = json.load(open('repos.json'))\n",
    "repo_names = [r['name'] for r in repos]\n",
    "has_repository = repo_name in repo_names\n",
    "if not has_repository:\n",
    "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos > repo_info.json' % repo_name)\n",
    "  repo_info = json.load(open('repo_info.json')) \n",
    "  repo_url = repo_info['clone_url']\n",
    "else:\n",
    "  for r in repos:\n",
    "    if r['name'] == repo_name:\n",
    "      repo_url = r['clone_url']\n",
    "  \n",
    "stream = open(\"/root/.config/hub\", \"r\")\n",
    "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
    "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
    "\n",
    "! git clone \"$repo_url_with_token\"\n",
    "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
    "! cd \"$repo_name\" && git add -A\n",
    "! cd \"$repo_name\" && git commit -m \"Add assignment 02 results\"\n",
    "! cd \"$repo_name\" && git push -u origin master\n",
    "\n",
    "sub_info = {\n",
    "    'student_id': student_id,\n",
    "    'student_name': student_name, \n",
    "    'repo_url': repo_url,\n",
    "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
    "    'dateime': str(time.time()),\n",
    "    'asg_name': asg_name\n",
    "}\n",
    "json.dump(sub_info, open('info.json', 'w'))\n",
    "\n",
    "Javascript(script_save)\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
    "\n",
    "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
    "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
    "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
    "\n",
    "print(\"##########################################\")\n",
    "print(\"Done! Submisson created, Please download using the bellow cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "mX9OFzaLtYu_",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Download Submission (Run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "id": "PUzTlnX1nS8X",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "files.download(submission_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "oJLRl0DL5aSO",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "LCQPkvV83Kn0",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "\n",
    "*   Kaggle competition [PUBG Finish Placement Prediction](https://www.kaggle.com/c/pubg-finish-placement-prediction)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_02.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
