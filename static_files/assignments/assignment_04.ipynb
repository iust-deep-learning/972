{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_04.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woU4Sodh6ND",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #4\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1398, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWitIy1viFuD",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "- **Assignment Due:**  1398/03/17 23:59\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- You must run this notebook on Google Colab platform; there are some dependencies to Google Colab VM for some of the libraries.\n",
        "- **Before starting to work on the assignment please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "Assignment Page: https://iust-deep-learning.github.io/972/assignments/04_nlp_intro\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl972/](https://groups.google.com/forum/#!forum/dl972/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ejALNiDCWnd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjwf6dWNCIQP",
        "colab_type": "text"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeLkOPE6Qwr7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = 0 #@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg04')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQvV4nljttN",
        "colab_type": "text"
      },
      "source": [
        "# 1. Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAn6YwIsCDir",
        "colab_type": "text"
      },
      "source": [
        "In any NLP task with neural networks involved, we need a numerical representation of our input (which are mainly words). A naive solution would be to use a huge one-hot vector with the same size as our vocabulary, each element representing one word. But this sparse representation is a poor usage of a huge multidimentional space as it does not contain any usefull information about the meaning and semantics of a word. This is where **word embedding** comes in handy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JBd9lm2JXxX",
        "colab_type": "text"
      },
      "source": [
        "##  1.1 What is word embedding?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdauOjEUJynO",
        "colab_type": "text"
      },
      "source": [
        "Embeddings are another way of representing vocabulary in a lower dimentional (compared to one-hot representation) continuous space. The goal is to have similar vectors for the words with similar meanings (so the elements of the vector actually carry some information about the meaning of the words). The question is, how are we going to achieve such representations?\n",
        "The idea is simple but elegant: **The words appearing in the same context are likely to have similar meanings.** \n",
        "\n",
        "So how can we use this idea to learn word vectors? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_76DLOfpkpt",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 How to train?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRnNtAVVpodh",
        "colab_type": "text"
      },
      "source": [
        "We are going to train a simple neural network with a single hidden layer to perform a certain task, but then we’re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer and use this hidden layer as our word representation vector.\n",
        "\n",
        "So lets talk about this \"fake\" task. We’re going to train the neural network to do the following: given a specific word (the input word),  the network is going to tell us the probability for every word in our vocabulary of being near to this given word (be one of its context words). So the network is going to look somthing like this (considering that our vocabulary size is 10000):\n",
        "\n",
        "<p align=\"center\"><img src=\"http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png\" width=\"800\"/>   \n",
        "</p>\n",
        "<p align=\"center\"><a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">[source]</a></p>\n",
        "\n",
        "\n",
        "By training the network on this task, the words which appear in similar contexts are forced to have similar values in the hidden layer since they are going to give similar outputs, so we can use this hidden layer values as our word representation. \n",
        "\n",
        "* This  approach is called **skip-gram**. There is another similar but slightly different approach called **CBOW**. Read about **CBOW** and explain its general idea:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2i-efGw4w-",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PV5JfK1w_FA",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 A practical challenge with softmax activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5vWhiK8xGXH",
        "colab_type": "text"
      },
      "source": [
        "Softmax is a very handy tool when it comes to probability distribution prediction problems, but it has its downsides when the number of the nodes grows too large. Let's look at softmax activation in our output layer:\n",
        "\n",
        "$$\n",
        "\\mathbf{S_{ij}} = \\frac {e^{W_{j}^T Y_{i-1}}}{\\sum_{j=1}^{N} e^{W_{j}^T Y_{i-1}}\\ } \\\n",
        "$$\n",
        "\n",
        "As you can see, every single output is dependent on the other outputs, so in order to compute the derivative with respect to any weight, all the other weights play a role! For a 10000 output size this results in milions of mathematical operations for a single weight update, which is not practical at all! \n",
        "\n",
        "* There are various techniques to solve this issue, like using **hierarchical softmax** or **NCE (Noise Contrastive Estimation)**. The original Word2vec paper proposes a technique called **Negative sampling**. Read about this technique and explain its general idea:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fydONiIv6KD1",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hVU_-rl6Oeo",
        "colab_type": "text"
      },
      "source": [
        "* Explain why is it called **Negative sampling**? What are these **Negative** samples?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ8uAAHj6bOJ",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiwXl7cw6cku",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Word2vec in code "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIbQZYAl7fRy",
        "colab_type": "text"
      },
      "source": [
        "There is a vergy good library called ***gensim*** for using word2vec in python. You can train your own word vectors on your own corpora or use available pretrained models. For example the following model is word vectors for a vocabulary of 3 million words and phrases trained on roughly 100 billion words from a **Google News** dataset with vector length of 300 features: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEtyOdznTu0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAfvuJXNVedw",
        "colab_type": "text"
      },
      "source": [
        "Lets load this model in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH2ydEbLSNLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "\n",
        "print (\"# of words\", len(model.vocab))\n",
        "print (\"# of vectors\", len(model.vectors))\n",
        "print (\"the first 10 elements of embedding vector for the word king:\",\n",
        "       model.vectors[model.vocab[\"king\"].index][:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykZB5onFVraJ",
        "colab_type": "text"
      },
      "source": [
        "As you can see it requires a huge amount of memory!\n",
        "\n",
        "* Use gensim library, find the 3 most similar words to each given following target word using ***similar_by_word*** method, find all these words embeddings, reduce their dimension to 2 using a dimension reduction algorithm (eg. t-SNE or PCA) and plot the results in a 2d-scatterplot: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7KMvqmpz3Bsg",
        "colab": {}
      },
      "source": [
        "target_words = [\"king\", \"horse\", \"blue\", \"apple\",\n",
        "                \"computer\", \"lion\", \"rome\", \"tehran\",\n",
        "                \"orange\", \"red\", \"army\", \"cat\",\n",
        "                \"asia\", \"mouse\"]\n",
        "\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qZkDhHwvjrS",
        "colab_type": "text"
      },
      "source": [
        "You can find the cosine similarity between two word vectors using ***similarity*** method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqb5oiq9wEt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('logitech', '/', 'cat', '->', model.similarity('logitech', 'cat'))\n",
        "print ('black', '/', 'criminal', '->', model.similarity('black', 'criminal'))\n",
        "print ('white', '/', 'criminal', '->', model.similarity('white', 'criminal'))\n",
        "print ('black', '/', 'offensive', '->', model.similarity('black', 'offensive'))\n",
        "print ('white', '/', 'offensive', '->', model.similarity('white', 'offensive'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP46--Z7xq1T",
        "colab_type": "text"
      },
      "source": [
        "* As you can see there is a meaningfull similarity between the word *logitech* (a provider company of personal computer and mobile peripherals) and the word *cat*,  even though they shouldn't have this much similarity. Explain why do you think this happens? Find more examples for this phenomenon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtQ19uxZy0bq",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoMJG977zDBM",
        "colab_type": "text"
      },
      "source": [
        "* It seems that words like *criminal* and *offensive*  are more similar to the word *black* rather than *white*. It is claimed that word2vec model trained on Google News suffers from gender, racial and religious biases. Explain why do you think this happens and find 4 more examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pQBaOLQ2YqJ",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixpmMEqY2fWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUkQbEvkH4F9",
        "colab_type": "text"
      },
      "source": [
        "Word vectors have some other cool properties, for example  we know the relation between the meanings of the two words \"man\" and \"woman\" is similar to the relation between words \"king\" and \"queen\". So we expect  $e_{queen} - e_{king} = e_{women} - e_{man}$   or     $e_{queen} = e_{king} + e_{women} - e_{man}$ .\n",
        "\n",
        "* Show whether the above equation holds or not by following these steps:\n",
        "\n",
        "\n",
        "1.   Extract the embedding vectors for these words. \n",
        "2.   Subtract the vector of \"man\" from vector of  \"woman\" and add the vector of \"king\"\n",
        "3.   Find the cosine similarity of the resulting vector with the vector for the word \"queen\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRn3BphdLa8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeDLckyY_Yuq",
        "colab_type": "text"
      },
      "source": [
        "# 2. Context representation using a window-based neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_xZO7UDvy2m",
        "colab_type": "text"
      },
      "source": [
        "From the previous section, we saw that word vectors can store a lot of semantic information in themselves. But can we solve an NLP task by just feeding them through a simple neural network? Assume we want to find all named entities in a given sentence (aka Named Entity recognition). For example, In \"I bought 300 shares of Apple Corp. in the last year\". We want to locate the word \"Apple\" and categorize it as an Organization entity. \n",
        "\n",
        "Obviously, a neural network cannot guess the type entirely based on a single word. We need to provide an extra piece of information to help the decision. This piece of information is called \"Context\" . We can decide if the word Apple is referring to the company or fruit by seeing it in a sentence (context). However, feeding a complete sentence through a network is inefficient as it makes the input layer really big even for a 10-word sentence (10 * 300 = 3000, assuming an embedding size of 300). \n",
        "\n",
        "To make training such network possible, we make the input only by including K surrounding neighbor words. hence, apple can be easily classified as a company by looking at the context window \\[ the, apple, corporation ]\n",
        "\n",
        "<br>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/01_window_based.jpg\" width=\"500\"/>  \n",
        "</p>\n",
        "<br>\n",
        "\n",
        "In a window-based classifier, every input sentence $X = [\\mathbf{x^{(1)}}, ... , \\mathbf{x^{(T)}}]$ with a label sequence $Y = [\\mathbf{y^{(1)}}, ..., \\mathbf{y^{(T)}}]$  is split into $T$ `<context window, center word label>` data points. We create a context window $\\mathbf{w^{(t)}}$ for every token $\\mathbf{x^{(t)}}$ in the original sentence by concatenating its k surrounding neighbors: $\\mathbf{w^{(t)}} = [\\mathbf{x^{(t-k)}}; ...; \\mathbf{x^{(t)}}; ...; \\mathbf{x^{(t+k)}}]$, therefore our new data point is created as $\\langle \\mathbf{w^{(t)}} , \\mathbf{y^{(t)}} \\rangle$.\n",
        "\n",
        "Having word case information might also help the neural network to find name entities with higher confidence. To incorporate casing, every token $\\mathbf{x^{(t)}}$ is augmented with feature vector $\\mathbf{c}$ representing such information: $\\mathbf{x^{(t)}} = [\\mathbf{e^{(t)}};\\mathbf{c^{(t)}}]$ where $\\mathbf{e^{(t)}}$ is the corresponding  embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL0vre-GArYG",
        "colab_type": "text"
      },
      "source": [
        "In this section, we aim to build a window based feedforward neural network on the NER task, and then analyze its limitations through a case study.\n",
        "\n",
        "Let's import some depencecies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxbjRjZQ-DWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/data.tar.gz\n",
        "! tar xvfz data.tar.gz > /dev/null\n",
        "\n",
        "from IPython.display import SVG\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from ner_util import read_dataset, convert_to_window_based, preprocess, LBLS, \\\n",
        "      UNK_TOK, plot_confusion_matrix, visualize_loss_and_acc\n",
        "from ag_news_util import read_ag_news, AG_NEWS_LBLS, create_model_input, create_vocab\n",
        "\n",
        "! pip install -q tqdm flair\n",
        "from tqdm import tqdm\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpAenO6yA1Gv",
        "colab_type": "text"
      },
      "source": [
        "And define the model's hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lpz7QhUg1QE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_NEIGHBORS = 1\n",
        "WINDOW_SIZE = 2 * NUM_NEIGHBORS + 1\n",
        "VOCAB_SIZE = 10*1000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_CLASSES = 5\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv1vVu4J8ewm",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFI72OhDopa",
        "colab_type": "text"
      },
      "source": [
        "As discussed earlier, we want to include the word casing information. Here's our desired function to encode the casing detail in d-dimensional vector. Words \"Hello\", \"hello\", \"HELLO\" and \"hELLO\" have four different casings. Your encoding should support all of them; In other words, the implemented function must return 4 different vectors for these inputs, but the same output for \"Bye\" and \"Hello\", \"bye\" and \"hello\", \"bYe\" and \"hEllo\", etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJNPI66BmPxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Default dimension for the casing vector. \n",
        "# You can change it to match your desiered encoding.\n",
        "CASING_DIM = 4\n",
        "\n",
        "def get_casing(word):\n",
        "  \"\"\"\n",
        "  Return the casing information in a numpy array.\n",
        "  \n",
        "  Args:\n",
        "    word(str): input word, E.g. Hello\n",
        "    \n",
        "  Returns:\n",
        "    np.array(shape=(CASING_DIM,)): encoded casing\n",
        "    \n",
        "  Hint: You might find the one-hot encoding useful.\n",
        "  \"\"\"\n",
        "  casing = np.zeros(shape=(CASING_DIM,))\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  assert casing.shape == (CASING_DIM,)\n",
        "  return casing \n",
        "\n",
        "print(\"case(hello) =\", get_casing('hello'))\n",
        "print(\"case(Hello) =\", get_casing('Hello'))\n",
        "print(\"case(HELLO) =\", get_casing('HELLO'))\n",
        "print(\"case(hEllO) =\", get_casing('hEllO'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riPpzrwpH5ix",
        "colab_type": "text"
      },
      "source": [
        "Describe two other features that would help the window-based model  to perform better (apart from word casing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQYWrd0xIs3Z",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rwox_HFcDy",
        "colab_type": "text"
      },
      "source": [
        "CONLL 2003[1] is a classic NER dataset; It has five tags per each word: `[PER, ORG, LOC, MISC, O]`,  where the label `O` is for words that have no named entities. We use this dataset to train our window-based model. Note that our split is different from the original one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmL6ZFt4L9KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First read the dataset\n",
        "train, valid, vocab = read_dataset(VOCAB_SIZE)\n",
        "print(\"# Dataset sample\")\n",
        "print(\"valid[0] = \", end='')\n",
        "pprint((' '.join(valid[0][0]), ' '.join(valid[0][1])))\n",
        "\n",
        "# Convert to window-based data points\n",
        "wtrain = convert_to_window_based(train, n=NUM_NEIGHBORS)\n",
        "wvalid = convert_to_window_based(valid, n=NUM_NEIGHBORS)\n",
        "print(\"\\n# Window based dataset sample\")\n",
        "print(\"wvalid[:7] = \")\n",
        "pprint(wvalid[:len(valid[0][1])])\n",
        "\n",
        "# Create a dictionary to lookup word ids \n",
        "tok2id = {w:i for i, w in enumerate(vocab)}\n",
        "\n",
        "# Process windowed dataset\n",
        "(w_train, c_train), y_train = preprocess(wtrain, tok2id, get_casing)\n",
        "(w_valid, c_valid), y_valid = preprocess(wvalid, tok2id, get_casing)\n",
        "\n",
        "print(\"\\n# Pre precessed dataset stats\")\n",
        "print(\"w_train.shape, c_train.shape, y_train.shape =\", w_train.shape, c_train.shape, y_train.shape)\n",
        "print(\"\\n# Pre precessed sample\")\n",
        "print(\"w_valid[0] =\", w_valid[0])\n",
        "print(\"c_valid[0] =\", c_valid[0])\n",
        "print(\"y_valid[0] =\", y_valid[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-DBbPAG8aI",
        "colab_type": "text"
      },
      "source": [
        "Download and construct pre-trained embedding matrix using Glove word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI_pkmuLzeAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget \"http://nlp.stanford.edu/data/glove.6B.zip\" -O glove.6B.zip && unzip glove.6B.zip\n",
        "\n",
        "word2vec = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "  for line in tqdm(f, total=400000):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWF0HyHRuaUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It is a good practice to initialize out-of-vocabulary tokens\n",
        "# with the embeddings' mean\n",
        "mean_embed = np.mean(np.array(list(word2vec.values())), axis=0)\n",
        "\n",
        "# Create the embedding matrix according to our vocabulary\n",
        "embedding_matrix = np.zeros((len(tok2id), EMBEDDING_DIM))\n",
        "for word, i in tok2id.items():\n",
        "  embedding_matrix[i] = word2vec.get(word, mean_embed)\n",
        "  \n",
        "print(\"embedding_matrix.shape =\", embedding_matrix.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzTT-YXn8Vhv",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDkJRR6lHI21",
        "colab_type": "text"
      },
      "source": [
        "Let's build the model. we recommend Keras functional API. Number of layer as well as their dimensions is totally up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4gqrr-PVmzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_window_based_ner_model():\n",
        "  window = Input(shape=(WINDOW_SIZE,), dtype='int64', name='window')\n",
        "  casing = Input(shape=(WINDOW_SIZE * CASING_DIM,), dtype='float32', name='casing')\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  output = None\n",
        "  model = Model([window, casing], output)\n",
        "  \n",
        "  return model "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs_qMNUhzrVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create and visualize the NER model\n",
        "ner_model = get_window_based_ner_model()\n",
        "ner_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "ner_model.summary()\n",
        "SVG(model_to_dot(ner_model,show_shapes=True).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70jbWu57HyBU",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cTp0DXd1Nlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model and visualize the traning at the end\n",
        "ner_model_hist = ner_model.fit(\n",
        "    [w_train, c_train], y_train, \n",
        "    epochs=10,\n",
        "    batch_size=BATCH_SIZE, \n",
        "    validation_data=([w_valid, c_valid], y_valid)\n",
        ")\n",
        "visualize_loss_and_acc(ner_model_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fra436XcQst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't forget to run this cell.\n",
        "# this is a deliverable item of your assignemnt\n",
        "ner_model.save(str(ASSIGNMENT_PATH / 'window_based_ner.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzw0JmjM885S",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RneiVsbrJVa_",
        "colab_type": "text"
      },
      "source": [
        "Now, It's time to analyze the model behavior. Here is an interactive shell that will enable us to explore the model's limitations and capabilities. Note that the sentences should be entered with spaces between tokens, and Use \"do n't\" instead of \"don't\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK2HEczS4ToC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Interactive Shell\n",
        "input_sentence = \"I bought 300 shares of Apple Corp in the last year .\"#@param {type:\"string\"}\n",
        "tokens = input_sentence.strip().split(\" \")\n",
        "input_example = [(tokens, [\"O\"] * len(tokens))]\n",
        "winput = convert_to_window_based(input_example)\n",
        "(w_pred, c_pred), _ = preprocess(winput, tok2id, get_casing)\n",
        "predictions = ner_model.predict([w_pred, c_pred])\n",
        "predictions = [LBLS[np.argmax(l)] for l in predictions]\n",
        "print_sentence(sys.stdout, tokens, None, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIXjTxqcTQ9c",
        "colab_type": "text"
      },
      "source": [
        "To further understand and analyze mistakes made by the model, let's see the confusion matrix: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwo_tYf_Tsy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "y_pred = ner_model.predict([w_valid, c_valid])\n",
        "y_pred_id = np.argmax(y_pred, axis=1)\n",
        "y_valid_id = np.argmax(y_valid, axis=1)\n",
        "print(\"\\n# Classification Report\")\n",
        "print(classification_report(y_valid_id, y_pred_id, target_names=LBLS))\n",
        "\n",
        "print(\"# Confusion Matrix\")\n",
        "cm = confusion_matrix(y_valid_id, y_pred_id)\n",
        "plot_confusion_matrix(cm, LBLS, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RImQxZ42T0_m",
        "colab_type": "text"
      },
      "source": [
        "Describe the window-based network modeling limitations by exploring its outputs. You need to support your conclusion by showing us the errors your model makes. You can either use validation set samples or a manually entered sentence to force the model to make an error.  Remember to copy and paste input/output from the interactive shell here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yQaf3U6V8Ps",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Ll8GvSZKOj",
        "colab_type": "text"
      },
      "source": [
        "# 3. BOW Sentence Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7xWHfwGE6X-",
        "colab_type": "text"
      },
      "source": [
        "We have shown arithmetic relations are present in the embedding space. For example  $e_{queen} = e_{king} + e_{women} - e_{man}$ . But are they strong enough for building a rich representation of a sentence? Can we classify a sentence according to the mean of its word's embeddings? In this section, we will find the answers to the above questions.\n",
        "\n",
        "Assume sentence $X = [\\mathbf{x^{(1)}}, ..., \\mathbf{x^{(N)}}]$ is given, then a sentence representation $\\mathbf{R}$ can be calculated as following:\n",
        "\n",
        "$$\n",
        "\\mathbf{R} = \\frac{1}{N} \\sum_{i=1}^{N} e_{x^{(i)}} \\ \\ \\mathbf{R} \\in \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "where $e_{x^{(i)}}$ is an embedding vector for the token $x^{(i)}$.\n",
        "\n",
        "<br>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/02_bag_of_word.jpg\" width=\"400\"/>  \n",
        "</p>\n",
        "\n",
        "Having such a simple model will enable us to analyze and understand its capabilities more easily. In addition, we will try one of the state-of-the-art text processing tools, called Flair, which can be run on GPUs. The task is text classification on the AG News corpus, which consists of news articles from more than 2000 news sources. Our split has 110K samples for the training and 10k for the validation set. Dataset examples are labeled with 4 major labels: `{World, Sports, Business, Sci/Tech}`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmyU-rdWn_ph",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2qz_KFkN-9H",
        "colab_type": "text"
      },
      "source": [
        "Often, datasets in NLP come with unprocessed sentences. As a deep learning expert, you should be familiar with popular text processing tools such as NLTK, Spacy, Stanford CoreNLP, and Flair. Generally, text pre-processing in deep learning includes Tokenization, Vocabulary creation, and Padding. But here we want to do one more step, NER replacement. Basically, we want to replace named entities with their corresponding tags. For example \"George Washington went to New York\" will be converted to \"\\<PERSON> went to \\<LOC>\"\n",
        "\n",
        "The purpose of this step is to reduce the size of vocabulary and support more words. This strategy is proved to be most beneficial when our dataset contains a large number of named entities, e.g. News dataset. \n",
        "\n",
        "Most pre-processing parts are implemented for you. You only need to fill the following function. Be sure to read the Flair documentations first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljEWdpDau3Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tagged_string(sentence):\n",
        "  \"\"\"\n",
        "  Join tokens and replace named enitites\n",
        "  Args:\n",
        "    sentence(flair.data.Sentence): An input sentence, containing list of tokens and their NER tag\n",
        "    \n",
        "  Returns:\n",
        "    output(str): A String of sentence tokens separated by spaces and \n",
        "        each named enitity is replaced by its Tag\n",
        "  \n",
        "  Hint: Check out flair tutorials, https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md\n",
        "      sentence.get_spans('ner'), sentence.tokens, token.idx and entity.tag might be helpful.\n",
        "  \"\"\"\n",
        "  output = \"\"\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuEYG05IVuSW",
        "colab_type": "text"
      },
      "source": [
        "Test your implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CRNWY_gk7Xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "s = Sentence('Chad asks the IMF for a loan to pay for looking after more than 100,000 refugees from conflict-torn Darfur in western Sudan.', use_tokenizer=True)\n",
        "tagger.predict(s)\n",
        "s_ner = get_tagged_string(s)\n",
        "assert s_ner == '<PERSON> asks the <ORG> for a loan to pay for looking after <CARDINAL> refugees from conflict-torn <GPE> in western <GPE> .'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIN1jX29oP2g",
        "colab_type": "text"
      },
      "source": [
        "Define model's hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYNJDmuCoPNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10*1000\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKC2avAsVxCO",
        "colab_type": "text"
      },
      "source": [
        "Process the entire corpus. It will approximately take 50 minutes. Please be patient. You may want to go for the next sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7kj6xPGS9sK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TAGGER_BATCH_SIZE = 512\n",
        "\n",
        "if 'tagger' not in dir() or tagger is None:\n",
        "  tagger = SequenceTagger.load('ner-ontonotes')\n",
        "\n",
        "def precoess_sents(lst):\n",
        "  output = []\n",
        "  for i in tqdm(range(0, len(lst), TAGGER_BATCH_SIZE)):\n",
        "    batch = [Sentence(x, use_tokenizer=True) for x in lst[i:i + TAGGER_BATCH_SIZE]]\n",
        "    tagger.predict(batch, mini_batch_size=TAGGER_BATCH_SIZE, verbose=False)\n",
        "    batch = [get_tagged_string(s).lower() for s in batch]\n",
        "    output += batch\n",
        "    \n",
        "  return output\n",
        "\n",
        "print(\"# Download and read dataset\")\n",
        "(train_sents, train_lbls), (valid_sents, valid_lbls) = read_ag_news()\n",
        "\n",
        "print(\"\\n# Replace named entities with their corresponding tags\")\n",
        "# We need to free the gpu memory due to some unknown bug in flair library\n",
        "del tagger; tagger = SequenceTagger.load('ner-ontonotes')\n",
        "import torch; torch.cuda.empty_cache()\n",
        "train_sents_ner = precoess_sents(train_sents)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "del tagger\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "torch.cuda.empty_cache()\n",
        "valid_sents_ner = precoess_sents(valid_sents)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "assert len(train_sents_ner) == len(train_lbls)\n",
        "assert len(valid_sents_ner) == len(valid_lbls)\n",
        "\n",
        "del tagger\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\n",
        "torch.cuda.empty_cache()\n",
        "del tagger\n",
        "\n",
        "print(\"# Processed dataset sample\")\n",
        "print(\"train_sents[0] =\", train_sents[0])\n",
        "print(\"train_sents_ner[0] =\", train_sents_ner[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI39Cv6UeFUM",
        "colab_type": "text"
      },
      "source": [
        "Create the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABGCMnSZeJcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First create the vocabulary\n",
        "vocab = create_vocab(train_sents_ner, VOCAB_SIZE)\n",
        "tok2id = {w:i for i, w in enumerate(vocab)}\n",
        "\n",
        "# It is a good practice to initialize out-of-vocabulary tokens\n",
        "# with the embedding matrix mean\n",
        "mean_embed = np.mean(np.array(list(word2vec.values())), axis=0)\n",
        "\n",
        "# Create the embedding matrix according to the vocabulary\n",
        "embedding_matrix = np.zeros((len(tok2id), EMBEDDING_DIM))\n",
        "for word, i in tok2id.items():\n",
        "  embedding_matrix[i] = word2vec.get(word, mean_embed)\n",
        "\n",
        "# Fill index 0 with zero values: padding word vector\n",
        "embedding_matrix[0] = np.zeros(shape=(EMBEDDING_DIM, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr17xtK5lJYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the model input\n",
        "x_train, y_train = create_model_input(train_sents_ner, tok2id, MAX_LEN), to_categorical(train_lbls, NUM_CLASSES)\n",
        "x_valid, y_valid = create_model_input(valid_sents_ner, tok2id, MAX_LEN), to_categorical(valid_lbls, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THm0hKbD0kYw",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U5soM5boyXZ",
        "colab_type": "text"
      },
      "source": [
        "Let's build the model. As always Keras functional API is recommended. Numeber of layer as well as their dimensionality is totally up to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPcC6afipbof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIAwLmt2uLgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BowModel(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(BowModel, self).__init__(name='bow')\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "  def call(self, words):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      words(Tensor): An input tensor for word ids with shape (?, MAX_LEN)\n",
        "    \"\"\"\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    return output\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xweAlVK9qs7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create and visualize the NER model\n",
        "bow_model = BowModel()\n",
        "bow_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9vved-2IZw",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59BC2ZTYDxWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and visualize training\n",
        "bow_model_hist = bow_model.fit(\n",
        "    x_train, y_train, \n",
        "    batch_size=BATCH_SIZE, epochs=10, \n",
        "    validation_data=(x_valid, y_valid)\n",
        ")\n",
        "visualize_loss_and_acc(bow_model_hist)\n",
        "bow_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB6PzYm804Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't forget to run this cell.\n",
        "# this is a deliverable item of your assignemnt\n",
        "bow_model.save_weights(str(ASSIGNMENT_PATH / 'bow_model.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ13xrXk1Djz",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibqSw5ds1Fks",
        "colab_type": "text"
      },
      "source": [
        "Same as the previous section, an interactive shell is provided. You can enter an input sequence to get the predicted label. The preprocessing functions will do the tokenization, thus don't worry about the spacing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia7loL9f3OP0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Interactive Shell\n",
        "if 'tagger' not in dir() or tagger is None:\n",
        "  tagger = SequenceTagger.load('ner-ontonotes')\n",
        "\n",
        "input_text = \"Chad asks the IMF for a loan to pay for looking after more than 100,000 refugees from conflict-torn Darfur in western Sudan.\"#@param {type:\"string\"}\n",
        "input_sents_ner = precoess_sents([input_text])\n",
        "input_tensor = create_model_input(input_sents_ner, tok2id, MAX_LEN)\n",
        "pred_label = bow_model.predict(input_tensor)\n",
        "\n",
        "print(\"\\n-----\\n\\n    x: \", input_text)\n",
        "print(\"x_ner: \", input_sents_ner[0])\n",
        "print(\"\\n   y': \", AG_NEWS_LBLS[np.argmax(pred_label[0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snJiqVoj40tu",
        "colab_type": "text"
      },
      "source": [
        "It is always helpful to see the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPRAa3A5Dtod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "yp_valid = bow_model.predict(x_valid)\n",
        "yp_valid_ids = np.argmax(yp_valid, axis=1)\n",
        "y_valid_ids = np.argmax(y_valid, axis=1)\n",
        "print(\"\\n# Classification Report\")\n",
        "print(classification_report(y_valid_ids, yp_valid_ids, target_names=AG_NEWS_LBLS))\n",
        "\n",
        "print(\"# Confusion Matrix\")\n",
        "cm = confusion_matrix(y_valid_ids, yp_valid_ids)\n",
        "plot_confusion_matrix(cm, AG_NEWS_LBLS, normalize=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FnPE59w5xv5",
        "colab_type": "text"
      },
      "source": [
        "Obviously, this is a relatively simple model. Hence it has limited modeling capabilities; Now it's time to find its mistakes. Can you fool the model by feeding a toxic example? Can you see the bag-of-word effect in its behavior? Write down the model limitation, Answers to the above questions, and keep in mind that you need to support each of your thoughts with an input/output example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLCCa7N8GzG",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u75b8GGLHl-S",
        "colab_type": "text"
      },
      "source": [
        "# 4. RNN Intuition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z9nQeiO1MlZ",
        "colab_type": "text"
      },
      "source": [
        "Up to now, we've investigated window-based neural networks and the bag-of-words model. Given their simple architectures, the representation power of these models mainly relies on the pre-trained embeddings. For example, a window-based model cannot understand the previous token's label which makes it struggle in identifying multi-word entities. While, adding a single word \"*not*\" can entirely change the meaning of a sentence, the BoW model is not sensitive to this as it ignores the order and computes the average embedding (in which single words do not play big roles).\n",
        "\n",
        "In contrast, RNNs read sentences word by word. At each step, the softmax classifier is forced to predict the label not only by using the input word but also using its context information. If we see the context information as a working memory for RNNs, it will be interesting to find what kind of information is stored in them while it parses a sentence.\n",
        "\n",
        "To visualize an RNN memory, we will train a language model on a huge chunk of text, and use the validation set to analyze its brain.  Then, we will watch each context neuron activation to see if it shows a meaningful pattern while it goes through a sentence. The following figure illustrates a random neuron in the memory which captures the concept of line length. It gradually turns off by reach the sentence end. Probably our model uses this neuron to handle \"\\n\" generation.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/03_sentence_length.jpg\" width=\"800\"/>   \n",
        "</p>\n",
        "<p align=\"center\"><a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">[source]</a></p>\n",
        "\n",
        "Here is another neuron which is sensitive when it's inside a quote.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/04_quotes.jpg\" width=\"800\"/>   \n",
        "</p>\n",
        "<p align=\"center\"><a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">[source]</a></p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0ktFZqrF1Oh",
        "colab_type": "text"
      },
      "source": [
        "Here, our goal is to find other meaningful patterns in the RNN hidden states. There is an open source library called LSTMVIs which provides pre-trained models and a great visualization tool. First, watch its tutorial and then answer the following questions: \n",
        "\n",
        "[LSTMVis](http://lstm.seas.harvard.edu)\n",
        "\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "For each model, find at least two meaningful patterns, and support your hypothesis with screenshots of LSTMVis.\n",
        "\n",
        "**1- [Character Model (Wall Street Journal)](http://lstm.seas.harvard.edu/client/lstmvis.html?project=18char&source=states::output2)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mdZ_CwxIrI9",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3KCXMQgI5k3",
        "colab_type": "text"
      },
      "source": [
        "**2- [Word Model (Wall Street Journal)](http://lstm.seas.harvard.edu/client/lstmvis.html?project=13ptb&source=states::output2)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89QHgJpWJNoN",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCqZFHLwJRe7",
        "colab_type": "text"
      },
      "source": [
        "3- Can you spot the difference between a character-based and a word-based language model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHuWNCB0JcQo",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBjq-MvamPXO",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o4q5LiFiOx1",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab downloaded file (`dl_asg04__xx__xx.zip`) and submit it via https://forms.gle/xTkMG2XhDgxTPwUf8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRUf35av3ZP",
        "colab_type": "text"
      },
      "source": [
        "**Note:** We need your Github token to create a new repository  (if it doesn't exist previously) in order to store learned model data. Also Google Drive token enables us to download the current notebook and Create the submission. If you are interested, feel free to check our code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTytERc-vlaK",
        "colab_type": "text"
      },
      "source": [
        "## Create Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf1s5OvZVGHI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_04'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg04__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "course_url = 'https://iust-deep-learning.github.io/972/'\n",
        "\n",
        "! tar xf hub-linux-amd64-2.10.0.tgz\n",
        "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
        "! hub config --global hub.protocol https\n",
        "! hub config --global user.email \"$Your_Github_account_Email\"\n",
        "! hub config --global user.name \"$student_name\"\n",
        "! hub api -X GET /user\n",
        "! hub api -X GET /user > user_info.json\n",
        "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
        "\n",
        "user_info = json.load(open('user_info.json'))\n",
        "repos = json.load(open('repos.json'))\n",
        "repo_names = [r['name'] for r in repos]\n",
        "has_repository = repo_name in repo_names\n",
        "if not has_repository:\n",
        "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos homepage=\"%s\" > repo_info.json' % (repo_name, course_url))\n",
        "  repo_info = json.load(open('repo_info.json')) \n",
        "  repo_url = repo_info['clone_url']\n",
        "else:\n",
        "  username = user_info['login']\n",
        "  ! hub api -F homepage=\"$course_url\" -X PATCH /repos/$username/$repo_name\n",
        "  for r in repos:\n",
        "    if r['name'] == repo_name:\n",
        "      repo_url = r['clone_url']\n",
        "  \n",
        "stream = open(\"/root/.config/hub\", \"r\")\n",
        "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
        "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
        "\n",
        "! git clone \"$repo_url_with_token\"\n",
        "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
        "! cd \"$repo_name\" && git add -A\n",
        "! cd \"$repo_name\" && git commit -m \"Add assignment 02 results\"\n",
        "! cd \"$repo_name\" && git push -u origin master\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'repo_url': repo_url,\n",
        "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
        "    'datetime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9OFzaLtYu_",
        "colab_type": "text"
      },
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUzTlnX1nS8X",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLRl0DL5aSO",
        "colab_type": "text"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQPkvV83Kn0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003.\n",
        "2. Zhang, Zhao, and LeCun, “Character-Level Convolutional Networks for Text Classification.”\n",
        "3. Stanford CS224d Course\n",
        "4. “The Unreasonable Effectiveness of Recurrent Neural Networks.” Accessed May 26, 2019. http://karpathy.github.io/2015/05/21/rnn-effectiveness/.\n"
      ]
    }
  ]
}