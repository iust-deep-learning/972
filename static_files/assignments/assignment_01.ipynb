{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_01.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "ByFVprjLlQS1",
        "DAxg5j543lc5",
        "zeDLckyY_Yuq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-woU4Sodh6ND",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment #1 - Deep Learning Fundamentals\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1398, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JWitIy1viFuD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "- **Assignment Due: ** 1397/12/24 23:59:00\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course Forum page.\n",
        "- You must run this notebook on Google Colab platform, it depends on Google Colab VM for some of the depencecies.\n",
        "- **Before starting to work on the assignment Please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "Assignment Page: [https://iust-deep-learning.github.io/972/assignments/01_deep_learning_fundamentals](https://iust-deep-learning.github.io/972/assignments/01_deep_learning_fundamentals)\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl972/](https://groups.google.com/forum/#!forum/dl972/)"
      ]
    },
    {
      "metadata": {
        "id": "0ejALNiDCWnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Vjwf6dWNCIQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "metadata": {
        "id": "NeLkOPE6Qwr7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Enter your information & \"RUN the cell!!\" { run: \"auto\" }\n",
        "student_id = 0 #@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg01')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ByFVprjLlQS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Classifying Persian Handwritten Digits \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ehvfTqCwlczI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In class, we studied English handwritten digit classification problem using Keras framework and trained a neural network on the MNIST dataset. In this assignment, we would like to dig a little deeper and become more familiar with the implementation details, Keras features, and preprocessing stage in a deep-learning pipeline."
      ]
    },
    {
      "metadata": {
        "id": "m2tvpBmdrfr2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Preprocessing Stage"
      ]
    },
    {
      "metadata": {
        "id": "u5Hr-5egxTuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The preprocessing of the dataset is one of the most important and time-consuming steps of any deep learning project. Most of the time, we have a dataset that is not well-processed and ready to be fed into a neural network. Sometimes, the data might even be clean and organized, but not suitable to fit to our problem. \n",
        "\n",
        "In this scenario, we have a dataset of Persian handwritten digits and we want to recognize digits only by looking at their image. Here is a brief stat of our dataset:"
      ]
    },
    {
      "metadata": {
        "id": "rMs7yQVNyOk-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "| Property | value\n",
        "| --- | --- |\n",
        "| Resolution of samples | 200 dpi |\n",
        "| Sample per each digit | ~10,000 samples |\n",
        "| Training samples: | 60,000 samples |\n",
        "| Test samples: | 20,000 samples |"
      ]
    },
    {
      "metadata": {
        "id": "vt7TK_Bhz0Vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's download and explore our dataset. Run the following commands:"
      ]
    },
    {
      "metadata": {
        "id": "YiM7sUg_0B7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "! wget -q http://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/data.zip\n",
        "  \n",
        "# Then, Extact it\n",
        "! unzip data.zip -d ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2BbxvdGw04kf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before going any further, we have to import some prerequisites:"
      ]
    },
    {
      "metadata": {
        "id": "IERram1b1rNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (7,9) # Make the figures a bit bigger\n",
        "\n",
        "from util import read_raw_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g7X6gse924WP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset using the provided function `read_raw_dataset`:"
      ]
    },
    {
      "metadata": {
        "id": "AHpIcci61suj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds_images, ds_labels = read_raw_dataset(\"train.cdb\")\n",
        "\n",
        "print(\"Images:\")\n",
        "print(type(ds_images))\n",
        "print(len(ds_images))\n",
        "print(type(ds_images[0]))\n",
        "print(ds_images[0].shape)\n",
        "\n",
        "print(\"\\nLabels:\")\n",
        "print(ds_labels[:30])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JSgvKnI84bsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, `train_images` is a list of images and `train_labels` contains the labels for each element in the image list. Each image is represented as 2D (numpy) array of floats/ints.\n",
        "\n",
        "So, lets look at some of the images:"
      ]
    },
    {
      "metadata": {
        "id": "VH4HPNnk5ZYC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Randomly sample images, Re run the cell to see new images.\n",
        "plt.figure(figsize=(12,8))\n",
        "for i, index in enumerate(random.sample(list(range(len(ds_images))), 10)):\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(ds_images[index], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Label {}\".format(ds_labels[index]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRHd2NXo6-M9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pretty great! ha? But it seems images are not in a fixed dimension. Let's check our hypothesis:"
      ]
    },
    {
      "metadata": {
        "id": "tnf_8zcG3m3a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unique_heights = list(set([m.shape[0] for m in ds_images]))\n",
        "print(\"unique heights:\", unique_heights)\n",
        "\n",
        "unique_widths = list(set([m.shape[1] for m in ds_images]))\n",
        "print(\"unique widths:\", unique_widths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D-KxKSAM9Zdh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unfortunately, the images in our dataset do not have equal dimensions. So, we have to implement a function to fit the image in a fixed frame. \n",
        "\n",
        "**Note**: There might be a situation where the image is bigger than the frame size; So, we first need to scale it down and then fit it into a fixed frame."
      ]
    },
    {
      "metadata": {
        "id": "tkG3GWo4346Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit_and_resize_image(src_image, dst_image_size):\n",
        "  \"\"\"\n",
        "  fit the image in fixed size black background & resize it if needed.\n",
        "  \n",
        "  Args:\n",
        "    src_image: 2d numpy array of image it may be in any shape\n",
        "    dst_image_size: size of square background frame\n",
        "    \n",
        "  Returns:\n",
        "    dst_image: 2d numpy array with shape (dst_image_size, dst_image_size). src_image should \n",
        "               be fitted in the center of the background.\n",
        "               \n",
        "  Hint: OpenCv.resize, np.zeors may be useful for you.\n",
        "  \"\"\"\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orXA-f88CQj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define a constant for our image size. please do not change it\n",
        "IMAGE_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GvwEzsP2BsjS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test your implementation using the following cell:"
      ]
    },
    {
      "metadata": {
        "id": "T2rJ91rJ1-56",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1); plt.imshow(ds_images[10], cmap='gray', interpolation='none'); plt.title('Before')\n",
        "plt.subplot(1,2,2); plt.imshow(fit_and_resize_image(ds_images[10], IMAGE_SIZE), cmap='gray', interpolation='none'); plt.title('After')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxk4hNEYEKjC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congradulations! You've successfully implemented the `fit_and_resize_image` function. Now it's time to use that and create a function to read the dataset, process it, and return it as a Numpy array. As you may have noticed, image values are in the 0-255 range (0 is black and 255 is white). Generally, we first normalize our input when we want to feed them into a neural network.  So, don't forget to normalize the images.\n",
        "\n",
        "**Note:** When we're solving a multi-label classification probelm it is important for the labels to be in one-hot format. So remember to convert them to one-hot format before returning the function."
      ]
    },
    {
      "metadata": {
        "id": "dLxlBwZFGB46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_dataset(dataset_path, images_size=32):\n",
        "  \"\"\"\n",
        "  Read & process Persian handwritten digits dataset\n",
        "  \n",
        "  Args:\n",
        "    dataset_path: path to dataset file\n",
        "    image_size: size that should be fixed for all images.\n",
        "    \n",
        "  Returns:\n",
        "    X: numpy ndarry with shape (num_samples, images_size, images_size) for normalized images \n",
        "    y: numpy ndarry with shape (num_samples, 10) for labels in one-hot format\n",
        "  \"\"\"\n",
        "  images, labels = read_raw_dataset(dataset_path)\n",
        "\n",
        "  X = np.zeros(shape=[len(images), images_size, images_size], dtype=np.float32)\n",
        "  Y = np.zeros(shape=[len(labels), 10], dtype=np.int)\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PZuHST5qHU9j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question:** Why should we normalize images to the [0-1] range?"
      ]
    },
    {
      "metadata": {
        "id": "phCuKiqJHWj4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "nnhkNT3GJhR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Great! Now we have everything ready to build our neural network."
      ]
    },
    {
      "metadata": {
        "id": "7iKd9HDtJrXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images, train_labels = read_dataset(\"train.cdb\", IMAGE_SIZE)\n",
        "test_images, test_labels = read_dataset(\"test.cdb\", IMAGE_SIZE)\n",
        "\n",
        "assert train_images.shape == (60000, IMAGE_SIZE, IMAGE_SIZE)\n",
        "assert test_images.shape == (20000, IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "assert train_labels.shape == (60000, 10)\n",
        "assert test_labels.shape == (20000, 10)\n",
        "\n",
        "assert train_images.mean() > 0.0\n",
        "assert test_images.mean() > 0.0\n",
        "\n",
        "assert 0. <= train_images.min() and train_images.max() <= 1\n",
        "assert 0. <= test_images.min() and test_images.max() <= 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxGV-DrCKGeB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "for i, index in enumerate(random.sample(list(range(len(train_images))), 6)):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.imshow(train_images[index], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Label {}\".format(str(np.argmax(train_labels[index]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QTKXFLRJKh2B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Building the model "
      ]
    },
    {
      "metadata": {
        "id": "Du2sqeqM5qPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In class, we exclusively used Keras Layers API to build a neural network; but, now, we want to get a little deeper and see what's going on behind this API."
      ]
    },
    {
      "metadata": {
        "id": "WkQqk_qy5q9Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Low level implementation"
      ]
    },
    {
      "metadata": {
        "id": "XbiDRyQc5yka",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we code in Keras, we are simply stacking up layer instances. However the implementation of neural networks is basically nothing but matrix multiplication and applying functions on them.  Deep learning libraries are made to perform matrix multiplication in the first place; so, each of these frameworks should be equipped with matrix manipulation tools and APIs. Keras is no exception."
      ]
    },
    {
      "metadata": {
        "id": "rc6oU07a_Avr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Perhaps you might think that Keras library has much higher level abstractions which makes significantly easier to work with. So, why do we have to learn low-level APIs? The point is that there are situations that we need low-level APIs. For example: \n",
        "* Sometimes the layers we want to use are not included in the default Keras implementation thus we have to implement them by ourselves.\n",
        "* More importantly, other deep learning frameworks such as Tensorflow and Pytorch are more commonly used in low-level mode. Learning such APIs will enable us to better understand the source codes of papers and existing projects available on the Internet."
      ]
    },
    {
      "metadata": {
        "id": "qkwjDUGSWn8n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before starting to code, let's review some of the mathematics behind neural networks. We store connection's weights between layers in the shape of a matrix. Also, we represent layer bais as a matrix too. Here is an example:"
      ]
    },
    {
      "metadata": {
        "id": "S4-i_iZPM-Yw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg01_assets/nn_weight.jpg\"  width=\"600\"/>"
      ]
    },
    {
      "metadata": {
        "id": "zqWXl6kNedTR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Suppose we have two layers in a neural network. Let $x \\in \\mathbb{R}^{N\\times1}$ be the activations of the previous layer with $N$ nodes, $W$ be the weight of connections and $B$ be the bais for the current layer. Write an algebraic equation for the current layer with $M$ nodes and activations $h$. Also, specify dimensions of matrices $W, B, h$"
      ]
    },
    {
      "metadata": {
        "id": "004Iu8lliQel",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "ZZUePpRvixsH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But, computers, especially GPUs, are much more efficient in parallel computing. So, instead of feeding single input $\\overrightarrow{x}$ through our neural network, why not feed them a batch? Thus, the input vector $\\overrightarrow{x} \\in \\mathbb{R}^{N\\times1}$ will be a matrix in the shape of $\\mathbb{R}^{b \\times N}$ ($b$ is the batch size) and each individual input will be one of $x$ rows.\n",
        "\n",
        "Rewritre equation for $h$ above and specify the dimensions of matrices $W, B, h$"
      ]
    },
    {
      "metadata": {
        "id": "EEmMlUpglofF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "wMZvcHEBsnIy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's import some prerequisites:"
      ]
    },
    {
      "metadata": {
        "id": "qReXYmHbtJLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Layer, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.utils.layer_utils import count_params\n",
        "\n",
        "from util import BaseModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rXVzUEr2zrSK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the model class that we're going to implement. `LowLevelMLP` is a simple MLP network with a configurable number of hidden layers.  To complete this section you need to fill its two methods `__init__()` and `build_model()`. We use  `__init__` to define networks weights and `build_model()` to specify operations between input and model weights to create the output.\n",
        "\n",
        "**Note**: Using `keras.layers` is not allowed in this part. Please review keras documents for [self.add_weight](https://keras.io/layers/writing-your-own-keras-layers/) and [K (Keras backend functions) ](https://keras.io/backend/#backend-functions)"
      ]
    },
    {
      "metadata": {
        "id": "8yfIadoJ9V_7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model configuration, Do not change\n",
        "HIDDEN_LAYERS = [512, 128]\n",
        "NUM_CLASSES = 10\n",
        "NUM_EPOCH = 20\n",
        "BATCH_SIZE = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5NuqTOYks4jg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LowLevelMLP(BaseModel):\n",
        "  \n",
        "  def __init__(self, input_shape, hidden_layers, num_classes=10):\n",
        "    \"\"\"\n",
        "    Initiate model with provided configuration\n",
        "    Args:\n",
        "      input_shape: size of input vector\n",
        "      hidden_layers: a list of integer, specify num hidden layer node from left to right,\n",
        "                     e.x.: [512, 128, ...]\n",
        "      num_classes: an integer defining number of classes, this is the number of model ouput nodes\n",
        "    \"\"\"\n",
        "    super(LowLevelMLP, self).__init__()\n",
        "    \n",
        "    self._input_shape = input_shape\n",
        "    self._hidden_layers = hidden_layers\n",
        "    self._num_classes = num_classes\n",
        "    \n",
        "    # Define model weights & biases according to self.hidden_layers and self.num_classes \n",
        "    # To create weight you can use self.add_weight\n",
        "    \n",
        "    self._model_weights = []\n",
        "    self._model_baiases = []\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    \n",
        "  def build_model(self, x):\n",
        "    \"\"\"\n",
        "    The Model logic sits here.\n",
        "    Args:\n",
        "      x: an input tensor in shape of (?, input_size), ? is batch size and will be determined at the training phase\n",
        "         e.x.: x is tensor with shape (?, 784) for the MNIST dataset\n",
        "\n",
        "    Returns:\n",
        "      pred: an output tensor with shape (?, self.num_classes)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Define operations between input and model weights here\n",
        "    # K.dot, K.relu, K.softmax might be useful for you.\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "\n",
        "ll_mlp = LowLevelMLP(IMAGE_SIZE ** 2, HIDDEN_LAYERS, NUM_CLASSES)\n",
        "ll_mlp._model_weights, ll_mlp._model_baiases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M_YoJSboGxyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "assert  count_params(ll_mlp.trainable_weights) == sum(\n",
        "    [(i*j) for i, j in zip([IMAGE_SIZE ** 2] + HIDDEN_LAYERS, HIDDEN_LAYERS + [NUM_CLASSES])] \n",
        "    + HIDDEN_LAYERS \n",
        "    + [NUM_CLASSES]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Av3RPPGw9o9T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train out model"
      ]
    },
    {
      "metadata": {
        "id": "1DZuL2qK9tHf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Before start training the model, we need to reshape the input so that each 32x32 image\n",
        "# becomes a single 1024 dimensional vector.\n",
        "\n",
        "x_train = train_images.reshape((-1, IMAGE_SIZE * IMAGE_SIZE))\n",
        "y_train = train_labels.astype('float32')\n",
        "\n",
        "x_test = test_images.reshape((-1, IMAGE_SIZE * IMAGE_SIZE))\n",
        "y_test = test_labels.astype('float32')\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTSdrJemBNka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ll_mlp.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "ll_mlp_history = ll_mlp.fit(x_train, y_train, epochs=NUM_EPOCH, batch_size=BATCH_SIZE, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ni4ja_DzU0lW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now visualize the traning:"
      ]
    },
    {
      "metadata": {
        "id": "MRwIX6uNVARh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_loss_and_acc(history):\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  acc = history_dict['acc']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  f = plt.figure(figsize=(10,3))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  acc_values = history_dict['acc']\n",
        "  val_acc = history_dict['val_acc']\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9BrqJQWTnLi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(ll_mlp_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ClYlNw_fF487",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remember to run this cell after each time you update the model, \n",
        "# this is one of deliverable items of your assignemnt\n",
        "ll_mlp.save_weights(ASSIGNMENT_PATH / 'll_mlp.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DexzFTGxVkLA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Custom Layer: Softmax"
      ]
    },
    {
      "metadata": {
        "id": "pP8a-KWtcBCy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this section, we're going to implement A custom Keras layer, A much realistic situation. For the sake of simplicity, we want to re-implement Softmax Layer. Before going through softmax implementation, let's review some of softmax details:\n",
        "\n",
        "Softmax has an interesting property that is quite useful in practice, Softmax is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$,\n",
        "\n",
        "$$\n",
        "\\mathrm{softmax(x) = softmax(x+c)}\n",
        "$$\n",
        "\n",
        "where $x + c$ means adding the constant c to every dimension of $x$. Remember that:\n",
        "\n",
        "$$\n",
        "softmax(x)_i = \\dfrac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "lvKrc53hVrKG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In practice, we make use of this property and choose $c = âˆ’ max_i\\  x_i$ when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of $x$)."
      ]
    },
    {
      "metadata": {
        "id": "JqsBtpLPfy9T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Softmax2D(Layer):\n",
        "  \"\"\"\n",
        "  Softmax activation function, Only works for 2d arrays.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Softmax2D, self).__init__(**kwargs)\n",
        "    # We don't have any configuration for this custom layer,\n",
        "    # But in future you should save any configuration related \n",
        "    # to your layer in its constructor\n",
        "    \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    \"\"\"Computes the output shape of the layer.\n",
        "\n",
        "    Assumes that the layer will be built\n",
        "    to match that input shape provided.\n",
        "\n",
        "    Args:\n",
        "      input_shape: Shape tuple (tuple of integers)\n",
        "        or list of shape tuples (one per output tensor of the layer).\n",
        "        Shape tuples can include None for free dimensions,\n",
        "        instead of an integer.\n",
        "\n",
        "    Returns:\n",
        "      An input shape tuple.\n",
        "    \"\"\"\n",
        "    # softmax of course doesn't change input shape, \n",
        "    # so we can simply return input_shape as output shape\n",
        "    return input_shape \n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    \"\"\"\n",
        "    This is where you will define your weights. \n",
        "    This method must set self.built = True at the end, \n",
        "    which can be done by calling super(Softmax2D, self).build().\n",
        "    \n",
        "    Args:\n",
        "      input_shape: Keras tensor (future input to layer)\n",
        "        or list/tuple of Keras tensors to reference\n",
        "        for weight shape computations.\n",
        "    \"\"\"\n",
        "    # As softmax is simple activation layer, we don't need any weight  \n",
        "    # definitions for this layer.\n",
        "    super(Softmax2D, self).build(input_shape)\n",
        "    \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    This is where the layer's logic lives.\n",
        "    \n",
        "    Args:\n",
        "      x: Input tensor, or list/tuple of input tensors.\n",
        "      \n",
        "    Returns:\n",
        "      A tensor.\n",
        "    \"\"\"\n",
        "    orig_shape = x.shape\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    assert x.shape[1] == orig_shape[1] and len(x.shape) == len(orig_shape)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JhdZb9JdozXE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test your implementation\n",
        "x = K.constant(np.array([[1001, 1002], [3, 4]]))\n",
        "test2 = K.eval(Softmax2D()(x))\n",
        "print(test2)\n",
        "ans2 = np.array([\n",
        "  [0.26894142, 0.73105858],\n",
        "  [0.26894142, 0.73105858]])\n",
        "assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
        "print(\"Passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fuP1ppSLpjZs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How to use our custom layer in practice?"
      ]
    },
    {
      "metadata": {
        "id": "Mh_HrSDDpjFn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create simple MLP network similar to what you implmented in previous section\n",
        "s2d_mlp = Sequential()\n",
        "s2d_mlp.add(Dense(512, activation='relu'))\n",
        "s2d_mlp.add(Dense(128, activation='relu'))\n",
        "s2d_mlp.add(Dense(NUM_CLASSES, activation=None))\n",
        "s2d_mlp.add(Softmax2D()) # This is your custom layer, \n",
        "\n",
        "# compile & train model\n",
        "s2d_mlp.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "s2d_history = s2d_mlp.fit(\n",
        "    x_train, y_train, \n",
        "    epochs=NUM_EPOCH, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-6QHyP_2P7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(s2d_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gScl_u643GLc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remember to run this cell after each time you update the model, \n",
        "# this is one of deliverable items of your assignemnt\n",
        "s2d_mlp.save(str(ASSIGNMENT_PATH / 's2d_mlp.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPriUD4_xRwb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Dropout"
      ]
    },
    {
      "metadata": {
        "id": "u5RPLAub0jm3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you see, the validation error of the model starts to grow around epoch 5. But, our network is being optimized on the training loss during this time and its error is continuously reduced if the training dataset is not big enough (which is the case here). The model has mastered the task on the training set, but the learned network doesn't generalize to new examples that it has never seen!\n",
        "\n",
        "Dropout is one of the most common regularization techniques used to prevent overfitting. It randomly shuts down some neurons in each iteration. \n",
        "\n",
        "**Question: ** Why Dropout helps model generalization (not overfitting)? What is the idea behind it? Explain with at least three reasons (feel free to use Google)."
      ]
    },
    {
      "metadata": {
        "id": "ABBfkdO-EPsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "HnzTgIReb4jE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Improving IMDB sentiment classifier\n",
        "\n",
        "Below is an MLP that is supposed to classify reviews in IMDB website as positive or negative (similar to what we saw in the class). But, this basic model starts to overfit very soon and, as a result, its performance on the validation set drops quickly. So, in this section, you need to improve this scenario. The following model is the base model; so, please do not change it. We will need it to compare our new model's performance."
      ]
    },
    {
      "metadata": {
        "id": "jH9pHNSuFftE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from util import load_imdb_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SGoHGwDRORLd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_imdb_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQsCMTxGRhxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imdb_mlp = Sequential()\n",
        "imdb_mlp.add(Dense(16, activation='relu', input_shape=(10000,)))\n",
        "imdb_mlp.add(Dense(16, activation='relu'))\n",
        "imdb_mlp.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "imdb_mlp.compile(optimizer='rmsprop', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "imdb_mlp_history = imdb_mlp.fit(x_train, \n",
        "                                y_train, \n",
        "                                epochs=20, \n",
        "                                batch_size=512, \n",
        "                                validation_data=(x_val, y_val))\n",
        "\n",
        "print(\"Accuracy on Test set is:\", imdb_mlp.evaluate(x_test, y_test)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TTbRXMSpWGM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(imdb_mlp_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PK8MiZDVWXSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, The model is overfitting quite fast! In this section you have to change the model, its prameters, and maybe add new layers to improve overfitting!"
      ]
    },
    {
      "metadata": {
        "id": "ukRJ6xNIWtjG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imdb_imprv = Sequential()\n",
        "\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "\n",
        "imdb_imprv.compile(optimizer='rmsprop', \n",
        "              loss='binary_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "imdb_imprv_history = imdb_imprv.fit(x_train, \n",
        "                                y_train, \n",
        "                                epochs=20, \n",
        "                                batch_size=512, \n",
        "                                validation_data=(x_val, y_val))\n",
        "\n",
        "print(\"Accuracy on Test set is:\", imdb_imprv.evaluate(x_test, y_test)[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2AM9_sBYbJRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Improved model:"
      ]
    },
    {
      "metadata": {
        "id": "AhOVNeCHbN9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(imdb_imprv_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMUAhddrbPuJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Base model:"
      ]
    },
    {
      "metadata": {
        "id": "we36GTJcbYaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(imdb_mlp_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F7Wry11vbbkL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question:** Briefly explain why your changes improved model performance on the test set."
      ]
    },
    {
      "metadata": {
        "id": "XOQWyTbPbuW7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "yVGC5E5agx10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remember to run this cell after each time you update the model, \n",
        "# this is one of deliverable items of your assignemnt\n",
        "imdb_imprv.save(str(ASSIGNMENT_PATH / 'imdb_imprv.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pBjq-MvamPXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ]
    },
    {
      "metadata": {
        "id": "7o4q5LiFiOx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instructions:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Make Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab downloaded file (`dl_asg01__xx__xx.zip`), compose an email to iust.dl972+asg01@gmail.com, Put the zip file in attachments and use `assignment_01` as title."
      ]
    },
    {
      "metadata": {
        "id": "iWRUf35av3ZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note: ** We need your Github token to create (if doesn't exist previously) new repository to store learned model data. Also Google Drvie token enable us to download current notebook & create submission. If you are intrested feel free to check our code."
      ]
    },
    {
      "metadata": {
        "id": "cTytERc-vlaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make Submission (Run the cell)"
      ]
    },
    {
      "metadata": {
        "id": "r4o37hc3AEUg",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_01'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg01__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "\n",
        "! tar xf hub-linux-amd64-2.10.0.tgz\n",
        "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
        "! hub config --global hub.protocol https\n",
        "! hub config --global user.email \"$Your_Github_account_Email\"\n",
        "! hub config --global user.name \"$student_name\"\n",
        "! hub api --flat -X GET /user\n",
        "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
        "\n",
        "repos = json.load(open('repos.json'))\n",
        "repo_names = [r['name'] for r in repos]\n",
        "has_repository = repo_name in repo_names\n",
        "if not has_repository:\n",
        "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos > repo_info.json' % repo_name)\n",
        "  repo_info = json.load(open('repo_info.json')) \n",
        "  repo_url = repo_info['clone_url']\n",
        "else:\n",
        "  for r in repos:\n",
        "    if r['name'] == repo_name:\n",
        "      repo_url = r['clone_url']\n",
        "  \n",
        "stream = open(\"/root/.config/hub\", \"r\")\n",
        "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
        "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
        "\n",
        "! git clone \"$repo_url_with_token\"\n",
        "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
        "! cd \"$repo_name\" && git add -A\n",
        "! cd \"$repo_name\" && git commit -m \"Add assignment 01 results\"\n",
        "! cd \"$repo_name\" && git push -u origin master\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'repo_url': repo_url,\n",
        "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
        "    'dateime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mX9OFzaLtYu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "metadata": {
        "id": "PUzTlnX1nS8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJLRl0DL5aSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- Khosravi, Hossein, and Ehsanollah Kabir. â€œIntroducing a Very Large Dataset of Handwritten Farsi Digits and a Study on Their Varieties.â€ Pattern Recogn. Lett. 28, no. 10 (July 2007): 1133â€“1141. https://doi.org/10.1016/j.patrec.2006.12.022.\n",
        "- Stanford Course: Deep Learning for NLP (CS224n)\n",
        "- Coursera Course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"
      ]
    }
  ]
}