{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_05.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woU4Sodh6ND",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #5\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1398, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWitIy1viFuD",
        "colab_type": "text"
      },
      "source": [
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "\n",
        "- **Assignment Due:**  1398/04/19 23:59\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- You must run this notebook on Google Colab platform; there are some dependencies to Google Colab VM for some of the libraries.\n",
        "- **Before starting to work on the assignment please fill your name in the next section AND Remember to RUN the cell.**\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "Assignment Page: https://iust-deep-learning.github.io/972/assignments/05_rnn_in_depth\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl972/](https://groups.google.com/forum/#!forum/dl972/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ejALNiDCWnd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjwf6dWNCIQP",
        "colab_type": "text"
      },
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeLkOPE6Qwr7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = 0 #@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg05')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QjjwwkNThk29"
      },
      "source": [
        "# 1. RNN in depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQjCMlHwjpeo",
        "colab_type": "text"
      },
      "source": [
        "We have investigated some text-based neural network in the previous assignment. Now we want actually to implement an advanced, research-ready LSTM network. We will train it in a memory-friendly setup, and bring *residual connection* and *pooling layer* from Computer vision to improve its performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIot0KTazvca",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 RNN input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m5fcwbc6W4r",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will go through all the details until we reach at a complete RNN pipeline implementation. Three conditions need to be taken into consideration when we're thinking about the RNN input. \n",
        "1. Formally, neural networks have inputs which are fixed in both size and shape, e.g., a 32x32 image, but in contrast inputs to an RNN network are variable-length sequences. \n",
        "2. Size and shape of any tensor in our neural network should be precisely defined at run time. \n",
        "\n",
        "So far, so good. We can feed one sample at a time to pass those conditions. But this is when the third condition appears.\n",
        "3. Generally, Stochastic Gradient Descent is referred to as a practical solution in the machine learning vocabulary, but actually, it is a theoretical approach which was first introduced in 1951 [1] as a stochastic gradient approximation technique, In other words, we cannot train a neural network with batch size = 1, we need more samples in each traning step to have a reasonable gradient approximation. So feeding one variable length input at a time will not work.\n",
        "\n",
        "As you may know, one possible solution is to pad the inputs to have the same lengths. First, we find the maximum possible length by looking at our dataset; then we pad any other samples with zeros to have the same length as the sample with maximum length. Here is an example from our codes in the class:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew-N3pv-WSHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 12\n",
        "MIN_LEN = 1\n",
        "\n",
        "# Create variable length sequence dataset\n",
        "seq_list = [[random.randint(0, 100) for _ in range(random.randint(MIN_LEN, MAX_LEN))] for _ in range(15)]\n",
        "print(\"# Un-padded dataset\")\n",
        "pprint(seq_list)\n",
        "\n",
        "# Create a padded version\n",
        "print(\"\\n # Padded version\")\n",
        "padded_seq_list = pad_sequences(seq_list, padding='post')\n",
        "padded_seq_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc7OpKlRYdcx",
        "colab_type": "text"
      },
      "source": [
        "Batching is also easy. We can directly create the chunks from `padded_seq_list`. All of them have a fixed `[batch_size, MAX_LEN]` shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLJlNKulZY7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "\n",
        "for i in range(0, len(padded_seq_list), BATCH_SIZE):\n",
        "  batch = padded_seq_list[i:i+BATCH_SIZE]\n",
        "  print(\"# Batch #%s, shape = %s\"%(i//BATCH_SIZE, batch.shape))\n",
        "  pprint(padded_seq_list[i:i+BATCH_SIZE])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om5TBcciaGw8",
        "colab_type": "text"
      },
      "source": [
        "It seems everything is alright, and we can jump straight into the implementation. But can you spot the huge waste of space? Do all of the batches have some sequence with the maximum length? Of course not! Let's imagine a real-world example; Generally, a text classification corpus has a large number of sentences. However, their length roughly follows the normal distribution (say $\\mu = 40$). That is, most of our sentences are within the distribution mean range. But always we have long sentences with a length even bigger than 100. Now you can guess how much space we would have lost if we had set the `MAX_LEN = 100`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU_Vc9RYgrN-",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Solution: Batching with variable `MAX_LEN`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojCOagxMhDIZ",
        "colab_type": "text"
      },
      "source": [
        "So instead of setting max_len to the dataset's maximum, we can set it for each individual batch. In other words, we will have different max_len across our batches. In this case, we not only pass the 3-fold conditions, but also are much more memory efficient. Let's rewrite the above example to illustrate this idea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZIEeciYiZsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 5\n",
        "\n",
        "for i in range(0, len(padded_seq_list), BATCH_SIZE):\n",
        "  batch = seq_list[i:i + BATCH_SIZE]\n",
        "  padded_batch = pad_sequences(batch, padding='post')\n",
        "\n",
        "  print(\"# Batch #%s, shape = %s\"%(i//BATCH_SIZE, padded_batch.shape))\n",
        "  pprint(padded_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL-5558krv2O",
        "colab_type": "text"
      },
      "source": [
        "As you can see, we've saved some memory in our artificial example. Let's jump back to our case when the sequences are real sentences. What does this input tensor really mean? Recall that the RNN input is a 3d tensor, but here we have 2d arrays. Actually, this 2d array is simply small number of sentences gathered together to construct one batch, and each word is replaced with its ID from the vocabulary \n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/01_lstm_input_2d.png\" width=\"850\"/>   \n",
        "</p>\n",
        "<p align=\"center\">Figure 1. Batch size = 3, and W<sub>i</sub> = Word ID assigned by the vocabulary, and 0 is reserved for padding; Here max_len = 6, and sentence 1 and 3 are padded to match up with the longest sentence</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wht4IR-O-B4n",
        "colab_type": "text"
      },
      "source": [
        "This 2d array is the actual input that we feed into our network. However, there is one more step. As mentioned earlier, an RNN input must be a 3d tensor. A typical input shape for an RNN is something like `[batch, max_len, features]`. Up to this point, we have `[batch, max_len]` with another dimension left. In NLP, features are simply the word embeddings. So let's bring our embedding layer to create the final tensor:\n",
        "\n",
        "</br>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/02_lstm_input_3d.png\" />   \n",
        "</p>\n",
        "</br>\n",
        "<p align=\"center\">Figure 2. E<SUB>i</SUB> is the embedding vector for word W<sub>i</sub>. This tensor has [batch = 3, max_len = 6, features = 300 ] shape, and is ready to be fed into an RNN network directly.  </p>\n",
        "\n",
        "Some time steps are grayed out by the masking layer. It simply hides these steps from the upcoming layers (here the upcoming layer is the RNN network). By masking these inputs we tell the RNN network to skip the grayed out inputs and continue the computation on the remaining sequences.\n",
        "\n",
        "As the figure denotes it, this 3D tensor is then split into its time steps so that each RNN step will receive a vertical 2d slice. With that said, the shape of an RNN cell input is `[batch, word_dim]` as well as its previous hidden state which has `[batch, hidden_dim] ` shape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdnp17cOSHPs",
        "colab_type": "text"
      },
      "source": [
        " ## 1.3 Batches with variable MAX_LEN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41T95o_jSOoE",
        "colab_type": "text"
      },
      "source": [
        "To implement the batching system with variable time-steps, we need to use handy tools that come with the Keras framework. As you might remember, the whole dataset has to be passed to `model.fit(...)` when we want to train the model. However, Keras provides another way to train our model. Indeed, `model.fit_generator(generator, ...)` is useful when we want to feed the data batches manually and one-by-one. \n",
        "\n",
        "As explained by [Keras documentations](https://keras.io/models/model/), the generator parameter must output a tuple `(inputs, targets)` where `inputs` are model's inputs, and `targets` are the labels corresponding to the inputs. This tuple forms a single batch. Naturally, the length for the input and output array should be similar for every batch but can have completely different sizes and shapes across all batches. Implement described batching system in the following class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzcSExW7bK9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.utils import Sequence\n",
        "\n",
        "\n",
        "class VariableLengthGenerator(Sequence):\n",
        "  \"\"\"Implements an efficient variable-length batch generator.\n",
        "    The value of Time step dimension is calculated accroding to the longest sequence \n",
        "    in each batch.\n",
        "  \"\"\"\n",
        "  def __init__(self, X, Y, batch_size, num_classes, shuffle=False):\n",
        "    \"\"\"\n",
        "    the class instructor.\n",
        "    Args:\n",
        "      X (list[list[int/float]]): A sequence input data\n",
        "      Y (list[int]): Target labels coresponding to ith element\n",
        "      batch_size (int): Pre-defined batch size\n",
        "      num_classes (int): Number of target classes. \n",
        "         To use with keras.utils.to_categorical\n",
        "      shuffle (boolean): Enable dataset shuffling after each epoch\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "  \n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Denotes the number of batches per epoch\n",
        "    Returns:\n",
        "      len (int): number of batches per epoch\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    return 0\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    Generate idx-th batch of dataset.\n",
        "    \n",
        "    Args:\n",
        "      idx (int): the batch index\n",
        "      \n",
        "    Returns:\n",
        "      A tuple: (\n",
        "        input batch (numpy.array(shape=[batch_size, batch_max_len])),\n",
        "        target batch (numpu.array(shape=[batch_size, num_classes], dtype=np.int))\n",
        "      )\n",
        "      \n",
        "    Hint: First find the sentence with the maximum length within the batch (batch_max_len). \n",
        "      then pad any other sentence with zero to match up with the batch_max_len\n",
        "    \"\"\"\n",
        "    \n",
        "    x = None\n",
        "    y = None\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    return (x, y)\n",
        "  \n",
        "  def on_epoch_end(self):\n",
        "    \"\"\"\n",
        "    This method is called after each epoch,\n",
        "      you can use it to shuffle dataset on every epoch.\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70DGzqmGp0Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test your implementation\n",
        "fake_input = [[1,3,4,3,4,5,2,3], [34,38,3], [93,499,5,95,3],[93,4,2,3], [4,3,94,4,5]]\n",
        "fake_target = [0,2,2,1,3]\n",
        "fake_gen = VariableLengthGenerator(fake_input, fake_target, 2, 5)\n",
        "\n",
        "batch_1_inputs, batch_1_targets = fake_gen[0]\n",
        "\n",
        "assert len(fake_gen) == 3\n",
        "assert batch_1_inputs.shape == (2, 8)\n",
        "assert batch_1_targets.shape == (2, 5)\n",
        "\n",
        "ref_batch_1_inp = np.array(\n",
        "    [[1,3,4,3,4,5,2,3],\n",
        "     [34,38,3,0,0,0,0,0]], dtype=np.int32)\n",
        "\n",
        "are_equal = ref_batch_1_inp == batch_1_inputs\n",
        "assert np.all(are_equal)\n",
        "\n",
        "ref_batch_1_tgts = np.array(\n",
        "    [[1,0,0,0,0],\n",
        "     [0,0,1,0,0]], dtype=np.int32)\n",
        "\n",
        "are_equal = ref_batch_1_tgts == batch_1_targets\n",
        "assert np.all(are_equal)\n",
        "print('Passed!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DquVku4so5c",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Residual LSTM cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQjFaatLSrP",
        "colab_type": "text"
      },
      "source": [
        "In neural networks, we believe that every new additional layer is adding more representation power to the model. More layers mean more abstractions and hopefully, better performance. But can we go deep and deeper as we want?\n",
        "Unfortunately the answer to this question is \"no\"; In practice, It is proven that naively making our model deep will always result in worse performance. The problem here is rooted in the training algorithm, i.e., Backpropagation. Having more layers result in more dimmed, vanished error signals. Therefore, early layers will have no idea about what they are doing wrong since the received error signal is quite faded.\n",
        "\n",
        "One of the simple but smart ideas to address this problem is Residual connections. It directly connects every single layer to the final output, which is considered as the primary source of the error signal. In other words, it makes the error signal propagation more efficient. In other words, it creates some kind of *highway* for neural network blocks to put their input on and receive high-quality error signal in the backward pass. In residual connections, ther input is added to the output along with neural network block result. i. e.  $y = F(x) + x$ where $F(x)$ is some neural network building block. \n",
        "\n",
        "</br>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/03_residual_connection.png\" width=\"400\" />   \n",
        "</p>\n",
        "\n",
        "<p align=\"center\">Figure 3. A residual connection with two neural network blocks.  </p>\n",
        "\n",
        "Residual connections were originally introduced in 2015 \\[2], primarily for image recognition networks. However, recurrent neural networks are no different from deep networks in terms of their deepness. In fact, RNNs unfold themselves to match their input sequence, which consequently creates a deep network. So, why not try residual connections on RNNs?\n",
        "\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/04_residual_lstm.png\" width=\"400\" />   \n",
        "</p>\n",
        "\n",
        "<p align=\"center\">Figure 4. Residual recuarrent neural network with multi layer LSTM cells.  </p>\n",
        "\n",
        "In an LSTM network, we can add residual connections between distinct layers to make dataflow simpler. Note that this connection is restricted to a single feedforward step of RNN. That is, LSTM cells cannot be connected across steps. It is intresting to know that Google has trained a huge deep neural machine translation system with 8 LSTM layers by incorporating this architecture for Google Translate. \n",
        "\n",
        "There are no residual connections in the first layer. Why? How can you fix this?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z9xIUtzFqKK",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHHJEeJ2HdQt",
        "colab_type": "text"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "In this section, our goal is to implement a Residual LSTM cell to be used with `keras.layer.RNN` class. `RNN` class is a handy tool from Keras library; it simply unfolds the network with the given **cell**. This **cell** parameter could be anything such as LSTM, GRU, or even SimpleRNN cell. We will leverage this functionality to create a residual wrapper around the LSTM cell. The idea for the wrapper is to first run the LSTM cell with the provided input and then create its output by summing over the input and the LSTM output.\n",
        "\n",
        "To make things easier, we skip the wrapper function and directly inherit our cell from `LSTMCell`. By using this approach, we'll lose some flexibility in exchange for convenience. Please remember the context for `call(..)` method is a single running RNN step, and its input is consequently a 2d slice of the whole RNN input which makes its shape in the form of  `[batch, features]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm7BjUuQM5zC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTMCell\n",
        "\n",
        "class ResidualLSTMCell(LSTMCell):\n",
        "  \"\"\"LSTM extension to with residual connection\"\"\"\n",
        "  \n",
        "  def __init__(self,units, **kwargs):\n",
        "      super(ResidualLSTMCell, self).__init__(units, **kwargs)\n",
        "      \n",
        "  def call(self, inputs, states, training=None):\n",
        "    \"\"\"Run single step of RNN.\n",
        "    \n",
        "    Args:\n",
        "      inputs (Tensor): input for this step x_t. Shape: [batch, features]\n",
        "      states: An lstm internal state data structure\n",
        "    \n",
        "    Returns:\n",
        "      (outputs, states): A tuple representing cell output, and the modified state\n",
        "       \n",
        "    Hints: call the parent class method, and then \n",
        "      create the final output from the original input and\n",
        "      actual lstm cell output. Do not touch the states parameter.\n",
        "    \"\"\"\n",
        "    \n",
        "    output = None\n",
        "    new_states = None\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    return output, new_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByHkX1BEbCrT",
        "colab_type": "text"
      },
      "source": [
        "Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ytbgKhQay5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "lcell = LSTMCell(3, activation=None, recurrent_activation=None, use_bias=True,\n",
        "                kernel_initializer='ones', recurrent_initializer='ones',\n",
        "                bias_initializer='zeros')\n",
        "\n",
        "rcell = ResidualLSTMCell(3, activation=None, recurrent_activation=None, use_bias=True,\n",
        "                kernel_initializer='ones', recurrent_initializer='ones',\n",
        "                bias_initializer='zeros')\n",
        "\n",
        "states = (K.constant(np.zeros([1,3])),K.constant(np.zeros([1,3])))\n",
        "inputs = K.constant(np.ones([1,3]))\n",
        "\n",
        "lcell.build(input_shape=(1, 3))\n",
        "rcell.build(input_shape=(1, 3))\n",
        "\n",
        "loutput, _ = lcell.call(inputs, states)\n",
        "routput, _ = rcell.call(inputs, states)\n",
        "\n",
        "assert K.eval(K.all(K.equal(routput-loutput,inputs)))\n",
        "print(\"Passed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyY9pF8XstSa",
        "colab_type": "text"
      },
      "source": [
        "## 1.5 Max-pooling through time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7Ecc6cQ1L9F",
        "colab_type": "text"
      },
      "source": [
        "Another technique that originates from Computer Vision is called Max pooling. As you might remember, this technique reduces the impact of spatial information in the image. For example, If your CNN says, \"Yay! I found a wheel at the position (x,y).\", your Max-pooling will convert this sentence to \"Yay! I found a wheel in this image.\"  Intuitively We can use the max-pooling procedure in any configuration beside an image. Here is an example of Max-pooling application in recurrent networks:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/05_rnn_max_pooling.png\" width=\"450\" />   \n",
        "</p>\n",
        "<p align=\"center\">Figure 5. Max-pooling through an RNN's hidden states.  </p>\n",
        "\n",
        "In this setup, we'd like to perform max-pooling over the hidden states $\\hat{h} = MaxPool([h^{(1)}, ..., h^{(n)}])$ where $\\hat{h}$ is the max-pooled version. Every dim of $\\hat{h}$ is the maximum of that particular dim across all of the hidden states.\n",
        "\n",
        "$$\n",
        "\\hat{h}_i =\\max_{1\\le k \\le n} h^{(k)}_i\n",
        "$$\n",
        "\n",
        "Explain why do you think max-pooling through time will help model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EH-ASSyo8eJ",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5ZDlY2OD4QH",
        "colab_type": "text"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MMzo2tyEPtr",
        "colab_type": "text"
      },
      "source": [
        "The default Keras framework provides the implementation but unfortunately it doesn't support masking. We need to max pool only over valid time steps. Implement this mechanism as a Keras layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3Y1lWjVX4XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Layer\n",
        "\n",
        "class MaskedGlobalMaxPooling1D(Layer):\n",
        "  \"\"\"\n",
        "  Max pooling over time steps\n",
        "  \"\"\"\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.supports_masking = True\n",
        "  \n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], input_shape[-1])\n",
        "\n",
        "  def compute_mask(self, input, input_mask=None):\n",
        "    return None\n",
        "  \n",
        "  def call(self, inputs, mask=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      inputs (Tensor(dtype=float32)): shape = [batch, timesteps, featurs]\n",
        "      mask (Tensor(dtype=bool)): shape = [batch, timesteps]\n",
        "      \n",
        "    Returns\n",
        "      output (Tensor(dtype=float32)): shape = [batch, features]\n",
        "      \n",
        "    Hint: K.tf.where, K.tile, K.max could be helpful.\n",
        "    \"\"\"\n",
        "    \n",
        "    output = None\n",
        "    \n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYr5iMRix-Dt",
        "colab_type": "text"
      },
      "source": [
        "Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWQpEjlhG6Ra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "seq = K.constant( [ \n",
        "    [[1, 3],\n",
        "     [4, -2],\n",
        "     [0, 11]],\n",
        "    \n",
        "    [[9, -3],\n",
        "     [8, -2],\n",
        "     [100, -5]]\n",
        "])\n",
        "mask = K.constant([\n",
        "    [True, True, True],\n",
        "    [True, True, False]\n",
        "], dtype='bool')\n",
        "\n",
        "max_pooled = MaskedGlobalMaxPooling1D()(seq, mask=mask)\n",
        "max_pooled = K.eval(max_pooled)\n",
        "\n",
        "print(max_pooled)\n",
        "assert np.all(max_pooled == np.array([[ 4., 11.],\n",
        "       [ 9., -2.]]))\n",
        "print(\"Passed!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaV5IrU_Mghj",
        "colab_type": "text"
      },
      "source": [
        "## 1.6 Trainable initial state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSsB5R0qSklU",
        "colab_type": "text"
      },
      "source": [
        "RNNs are stateful neural networks. They use this state to represent and store context information. They can add, delete, and modify their states in each step; yet, the initial value for this state vector is kind of unknown. A common approach is to start it with all-zero tensor. However, it is proved that this approach has some drawbacks. Another method is to consider the initial state as a trainable parameter, which means that the initial state can likely reach an optimal value to start the RNN.\n",
        "\n",
        "Fortunately You can specify the initial state in keras `RNN` class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5On8r12ES2fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.layers import Input, RNN, LSTMCell, Lambda, concatenate\n",
        "\n",
        "word_dim = 300\n",
        "hidden_dim = 200\n",
        "\n",
        "inp = Input((None, word_dim))\n",
        "\n",
        "# Fake identity layer to use its add_weights(...) method\n",
        "# We need to create this layer because of Keras limitations\n",
        "iden = Lambda(lambda x: x)\n",
        "x = iden(inp)\n",
        "batch_size = K.shape(x)[0]\n",
        "\n",
        "def create_trainable_initial_state(name_prefix):\n",
        "  h = iden.add_weight('%s_h' % name_prefix, shape=(1, hidden_dim), initializer='zeros')\n",
        "  c = iden.add_weight('%s_c' % name_prefix, shape=(1, hidden_dim), initializer='zeros')\n",
        "\n",
        "  # Make sure they match with shape [batch_size, hidden_dim]\n",
        "  h = K.tile(h, [batch_size, 1])\n",
        "  c = K.tile(c, [batch_size, 1])\n",
        "\n",
        "  return h, c\n",
        "\n",
        "# Create trainable variables for initial states\n",
        "# Forward\n",
        "fw_h, fw_c = create_trainable_initial_state('fw')\n",
        "# Backward\n",
        "bw_h, bw_c = create_trainable_initial_state('bw')\n",
        "\n",
        "# Create simple single layer RNN\n",
        "fw_rnn = RNN(LSTMCell(hidden_dim), return_sequences=True)\n",
        "bw_rnn = RNN(LSTMCell(hidden_dim), return_sequences=True, go_backwards=True)\n",
        "y_fw = fw_rnn(x, initial_state=[fw_h, fw_c])\n",
        "y_bw = bw_rnn(x, initial_state=[bw_h, bw_h])\n",
        "y = concatenate([y_fw, y_bw], axis=-1)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huEh9fQJs5XT",
        "colab_type": "text"
      },
      "source": [
        "## 1.7 Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZQaX8PzIKm1",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elvuaq-zIuJw",
        "colab_type": "text"
      },
      "source": [
        "Having all parts ready, we can now build our advanced deep memory efficient RNN classifier. You should first implement a multi-layer LSTM bidirectional recurrent neural network with residual connections. Set its initial states to a trainable parameter. Then put Max-pooling layer on top of that in order to reduce all hidden states in a single vector, and finally Feed the computed vector into a multi-layer dense network to get the final predicted label. Here is a complete figure for our advanced model:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg05_assets/06_final_network.png\" width=\"800\" /> </p>\n",
        "\n",
        "\n",
        "Hints:\n",
        "1. Create an Embedding layer to convert word ids to thier corresponding embeddings. Use `layers.Embedding` class and remember to pass `mask_zero=True` in its paramters.\n",
        "2. Create the RNN part:\n",
        " 1. Unfortunately Keras `Bidirectional` layer is corrupted. You need to create two separate RNNs, one for forward (`fw_rnn = RNN(..., goes_backwards=False)`) and one for the backward direction (`bw_rnn = RNN(..., goes_backwards=True)`).\n",
        " 2. Make sure that you've created two sets of Residual LSTM cells (`fw_cells, bw_cells`) and two sets of initial states (`fw_initial_states, bw_initial_states`).\n",
        " 3. Then concatenate both hidden states to create the final output of a bidirectional multi-layer residual LSTM network (you may find `keras.layers.concatenate([...], axis=-1) useful`.\n",
        "3. Reduce hidden states to the max pooled version via our `MaskedGlobalMaxPooling1D` layer.\n",
        "4. Create a dense multilayer network to predict the output label from the max-pooled hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vLK71W9fVx2",
        "colab_type": "text"
      },
      "source": [
        "Hyperpramaneters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kzQVcBEfT47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_NUM = 128\n",
        "NUM_RNN_LAYERS = 3\n",
        "EMBEDDING_DIM = 300\n",
        "DENSE_HIDDEN_LAYERS = [64, 32]\n",
        "DROPOUT_RATE = 0.4\n",
        "VOCAB_SIZE = 10*1000\n",
        "BATCH_SIZE = 128\n",
        "NUM_CLASSES = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJPHu2mfG5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Lambda\n",
        "\n",
        "def create_advanced_rnn(embedding_matrix, num_classes):\n",
        "  \"\"\"Create an advanced LSTM text classifier\n",
        "  \n",
        "  Args:\n",
        "    embedding_matrix (np.array(shape=(vocab, EMBEDDING_DIM))): word embedding matrix\n",
        "    num_classes (int): num of tar classes\n",
        "    \n",
        "  Returns:\n",
        "    model (keras.Model)\n",
        "  \"\"\"\n",
        "  # Input specs\n",
        "  x = Input((None,))\n",
        "  \n",
        "  # Fake layer to add any additional weights\n",
        "  iden = Lambda(lambda x:x)\n",
        "  \n",
        "  # The real input, shape: [batch_size, time_steps]\n",
        "  word_ids = iden(x)\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  y = None\n",
        "  model = Model(inputs=x, outputs=y)  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC7dLYG_s6AF",
        "colab_type": "text"
      },
      "source": [
        "Check your implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3GbJQKCy7hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "m = create_advanced_rnn(np.zeros((10, EMBEDDING_DIM)), 5)\n",
        "m.summary()\n",
        "\n",
        "SVG(model_to_dot(m).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDiZQ-FxY_BX",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-z6OoCFy_7t",
        "colab_type": "text"
      },
      "source": [
        "Similarly to the previous assignment, we will use AG News dataset to train our model. Here a description from assignment 04:\n",
        "\n",
        "`The task is text classification on the AG News corpus, which consists of news articles from more than 2000 news sources. Our split has 110K samples for the training and 10k for the validation set. Dataset examples are classified into 4 major topics: {World, Sports, Business, Sci/Tech}`\n",
        "\n",
        "</br>\n",
        "\n",
        "Let's download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFuaDvw4ZDhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget -q https://iust-deep-learning.github.io/972/static_files/assignments/asg04_assets/data.tar.gz\n",
        "! tar xvfz data.tar.gz > /dev/null\n",
        "\n",
        "from ag_news_util import read_ag_news, AG_NEWS_LBLS\n",
        "\n",
        "! pip install -q tqdm flair\n",
        "from tqdm import tqdm\n",
        "from flair.data import Sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pGGuKFU0MJE",
        "colab_type": "text"
      },
      "source": [
        "In contrast to assignment 04, the processing step is minimal here; we just need to tokenize sentences and convert words to their lower case version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51vmcPH5a94m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sent):\n",
        "  sent = Sentence(sent, use_tokenizer=True)\n",
        "  str_tokens = [t.text.lower() for t in sent.tokens]\n",
        "  return str_tokens\n",
        "\n",
        "(train_sents, train_lbls), (valid_sents, valid_lbls) = read_ag_news()\n",
        "\n",
        "train_tok_sents = list(map(tokenize, train_sents))\n",
        "valid_tok_sents = list(map(tokenize, valid_sents))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd97Fzof0zCo",
        "colab_type": "text"
      },
      "source": [
        "Download glove pre-trained word embedddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi2zfuIpgK6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget \"http://nlp.stanford.edu/data/glove.6B.zip\" -O glove.6B.zip && unzip glove.6B.zip\n",
        "\n",
        "word2vec = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "  for line in tqdm(f, total=400000):\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = coefs\n",
        "\n",
        "print('Found %s word vectors.' % len(word2vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDfv-Mck08qW",
        "colab_type": "text"
      },
      "source": [
        "Replace tokens with their ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEcWnLJfc_u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "PAD_TOK = '<pad>'\n",
        "UNK_TOK = '<unk>'\n",
        "special_toks = [PAD_TOK,UNK_TOK]\n",
        "\n",
        "def create_vocab(lst, vocab_size):\n",
        "  c = Counter()\n",
        "  for s in lst:\n",
        "    c.update(s)\n",
        "  \n",
        "  words = [w for w,_ in c.most_common(vocab_size)] \n",
        "  vocab = special_toks + words\n",
        "  \n",
        "  return vocab\n",
        "\n",
        "# First create the vocabulary\n",
        "vocab = create_vocab(train_tok_sents, VOCAB_SIZE)\n",
        "tok2id = {w:i for i, w in enumerate(vocab)}\n",
        "\n",
        "# It is a good practice to initialize out-of-vocabulary tokens\n",
        "# with the embedding matrix mean\n",
        "mean_embed = np.mean(np.array(list(word2vec.values())), axis=0)\n",
        "\n",
        "# Create the embedding matrix according to the vocabulary\n",
        "embedding_matrix = np.zeros((len(tok2id), EMBEDDING_DIM))\n",
        "for word, i in tok2id.items():\n",
        "  embedding_matrix[i] = word2vec.get(word, mean_embed)\n",
        "\n",
        "# Fill index 0 with zero values: padding word vector\n",
        "embedding_matrix[0] = np.zeros(shape=(EMBEDDING_DIM, ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2cpsze61ClB",
        "colab_type": "text"
      },
      "source": [
        "And finally, use the `VariableLengthGenerator` to create a variable length input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ6lWzG7kz_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk_id = tok2id[UNK_TOK]\n",
        "\n",
        "x_train = [[tok2id.get(w, unk_id) for w in s] for s in train_tok_sents]\n",
        "x_valid = [[tok2id.get(w, unk_id) for w in s] for s in valid_tok_sents]\n",
        "\n",
        "train_gen = VariableLengthGenerator(x_train, train_lbls, BATCH_SIZE, NUM_CLASSES, shuffle=True)\n",
        "valid_gen = VariableLengthGenerator(x_valid, valid_lbls, BATCH_SIZE, NUM_CLASSES, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bpsoYWb1auq",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZolEvZayRBN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title def visualize_loss_and_acc(history): ... RUN ME!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_loss_and_acc(history):\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  acc = history_dict['acc']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  f = plt.figure(figsize=(10,3))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  acc_values = history_dict['acc']\n",
        "  val_acc = history_dict['val_acc']\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSubO6N011Sz",
        "colab_type": "text"
      },
      "source": [
        "Create an instance of our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1NRienOm6ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "advance_rnn = create_advanced_rnn(embedding_matrix, NUM_CLASSES)\n",
        "advance_rnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "advance_rnn.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfJigqdu16rs",
        "colab_type": "text"
      },
      "source": [
        "Train and visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxJgI1fru0au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "advance_rnn_hist = advance_rnn.fit_generator(\n",
        "    train_gen, \n",
        "    validation_data=valid_gen,\n",
        "    epochs=10,\n",
        ")\n",
        "visualize_loss_and_acc(advance_rnn_hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaVziYVHT50Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't forget to run this cell.\n",
        "# this is a deliverable item of your assignemnt\n",
        "advance_rnn.save_weights(str(ASSIGNMENT_PATH / 'advance_rnn.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky2-nHbiqvFK",
        "colab_type": "text"
      },
      "source": [
        "### Analysis (Bonus point)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib4V47w5tJSp",
        "colab_type": "text"
      },
      "source": [
        "As our tradition, an interactive shell is also provided. Tokenization and pre-processing are done for you. You can easily explore the model's output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1TXpFmFq-ml",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Interactive Shell\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "\n",
        "input_sentence = \"Chad asks the IMF for a loan to pay for looking after more than 100,000 refugees from conflict-torn Darfur in western Sudan\"#@param {type:\"string\"}\n",
        "tokens = tokenize(input_sentence)\n",
        "word_ids = np.array( [[tok2id.get(w, unk_id) for w in tokens]])\n",
        "\n",
        "lbl_index = np.argmax(advance_rnn.predict(word_ids)[0])\n",
        "print(\"input tokens:\", tokens)\n",
        "print(\"predicted label:\", AG_NEWS_LBLS[lbl_index])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjPZg89utKVd",
        "colab_type": "text"
      },
      "source": [
        "Now we have a relatively complex model, one of the ways that we can compare this new model to the old bag-of-word network is to see if the complex model can handle the old model's mistakes. Find a misclassified example by the BOW model, which can be categorized correctly by this model. Are there any examples for which the former model can find the correct label while the newer one is unable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QVt4KTduk70",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzgM-v9lqlq",
        "colab_type": "text"
      },
      "source": [
        "# 2. Byte Pair encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdc5y3-Gw-q5",
        "colab_type": "text"
      },
      "source": [
        "In many natural language processing tasks, our vocabulary is limited to the vocabulary of our pretrained word embeddings and we have to replace words which are not present in our embedding vocabulary  with `UNK` token. There is a technique caled **Byte pair encoding** which proposes training the embeddings on a bunch of common subword units (e.g. learning embeddings of ##solu and ##tion instead of the word solution). **BPE** creates a list of merges that are used for splitting out-of-vocabulary words. You can find the details of how to find these common subwords and how to find the optimal segmentation for each word here in the original paper  https://arxiv.org/abs/1508.07909 .  We are not going into the detail of the algorithm, instead we are going to just use it and compare the results with the regular case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7DPTPHfwtBR",
        "colab_type": "text"
      },
      "source": [
        "##BPEmb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dy9BHLIw3_P",
        "colab_type": "text"
      },
      "source": [
        "BPEmb is a collection of pre-trained subword embeddings in 275 languages, based on Byte-Pair Encoding (BPE) and trained on Wikipedia.\n",
        "\n",
        "Let's Install BPEmb with pip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJr2gTiNxIvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install bpemb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNel7A5cxRTa",
        "colab_type": "text"
      },
      "source": [
        "You can do two main things with BPEmb. The first is subword segmentation, and the second purpose of BPEmb is to provide pretrained subword embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD50Zwxixif9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bpemb import BPEmb\n",
        "from pprint import pprint \n",
        "bpemb_en = BPEmb(lang=\"en\", dim=50)\n",
        "\n",
        "print (\"segmentation of word \\'stratford\\': \", bpemb_en.encode(\"stratford\"))\n",
        "print (\"\\n\")\n",
        "\n",
        "# Embeddings are wrapped in a gensim KeyedVectors object\n",
        "print (\"bpemb.emb is a \", type(bpemb_en.emb))\n",
        "print (\"\\n\")\n",
        "\n",
        "# You can use BPEmb objects like gensim KeyedVectors\n",
        "print (\"most similar words to \\'ford\\': \")\n",
        "pprint (bpemb_en.most_similar(\"ford\"))\n",
        "print (\"\\n\")\n",
        "print (\"bpemb.vectors is a \", type(bpemb_en.vectors))\n",
        "\n",
        "print (\"\\n\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVt7u-r90C8t",
        "colab_type": "text"
      },
      "source": [
        "To use subword embeddings in your neural network, either encode your input into subword IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJo4dpIE0Ik6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ids = bpemb_en.encode_ids(\"stratford\")\n",
        "print (\"ids :\", ids)\n",
        "print (\"\\n\")\n",
        "\n",
        "print (\"embeddings: \")\n",
        "pprint (bpemb_en.vectors[ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAHe34-0nKM",
        "colab_type": "text"
      },
      "source": [
        "Or use the `embed` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gEQbOcc0sgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"embeddings: \")\n",
        "pprint (bpemb_en.embed (\"stratford\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns2A3NVM1Rqs",
        "colab_type": "text"
      },
      "source": [
        "##Use BPEmb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ih9_ZdJ1WUZ",
        "colab_type": "text"
      },
      "source": [
        "Retrain your previuosly implemented RNN network in question1, this time using byte pair embedding instead of glove-word vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYyNCkMM2jWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI7ae3S32p5H",
        "colab_type": "text"
      },
      "source": [
        "compare the result with previous one and explain why it gets better/worse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuS_bVHI22rW",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hSzcqgzluB8",
        "colab_type": "text"
      },
      "source": [
        "# 3. Time Series Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQoJi3q_I6fg",
        "colab_type": "text"
      },
      "source": [
        "Due to the sequential charasteristics of RNNs, they can be used for time series prediction task. A time series is a series of data points indexed in time order. One example of time series is stock prices at regular intervals of time (hourly, daily, etc.). There have been many attempts to forecast stocks market, some belive predicting the market is impossible because of its total random behaviour, while some (mainly stocks technical analysts) belive not 100% accurate, but it is partly predictable, as they say \"history repeats itself\" (this phrase indicates that there might be some repetitive patterns in market's behaviour). \n",
        "\n",
        "In this practice, we are going to discover ourselves whether it is possible or not, whether we are going to be rich or broke :)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBLpUtW_5Q9K",
        "colab_type": "text"
      },
      "source": [
        "## Stocks Prediction Using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfO4ddWV5ZLS",
        "colab_type": "text"
      },
      "source": [
        "LSTMs can be used for modeling time series (such as stocks market) as they are capable of remembering past information. Stock price prediction is similar to any other machine learning problem where we are given a set of features and we have to predict a corresponding value, in this case our features are the closing prices of stocks in the past 60 days and we want to predict next closing price. We can use LSTM to model this 60 days sequence by a hidden state as we do in all other sequence modeling problems:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1hml7fG9_GKDLH1068Wq_Jk-D09FtQk17&sz=w1000-h1000\" width=\"800\"/></p>\n",
        "\n",
        "</br>\n",
        "\n",
        "We are going to solve these two following problems:\n",
        "* 1 . Predicting the exact price at time $T_i$ given prices at times $T_{i-60}$, ..., $T_{i-1}$\n",
        "\n",
        "* 2 . A simplified version of previous problem, predicting the direction of price at time $T_i$ (whether it is increasing or decreasing) given prices at times $T_{i-60}$, ..., $T_{i-1}$\n",
        "\n",
        "\n",
        "So the first task is a regression and the second one is a classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI0Ub0KuK0wG",
        "colab_type": "text"
      },
      "source": [
        "### 1. Predicting The Exact Price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSNVvB-apMAr",
        "colab_type": "text"
      },
      "source": [
        "**DATA:**\n",
        "\n",
        "The data that we are going to use are from ***Yahoo Finance*** (a place to look up stock market data and company news). We can accsess historical market data from *Yahoo Finance* using python ***yfinance*** module. \n",
        "\n",
        "Let's install *yfinance* module first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zYyq0SlbY_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install yfinance\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAwSxCbbWlt",
        "colab_type": "text"
      },
      "source": [
        "For training our algorithm, we will be using the Apple stock prices from 1st January 2014 to 1st January 2018. In order to evaluate the performance of the algorithm, we will be using the prices from 1st January 2018 to 1st January 2019. \n",
        "\n",
        "Let's get the data and visualise it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Q70Xcr3nNR",
        "colab_type": "code",
        "outputId": "d6b68bfd-b47d-4aeb-8ac8-6d4647a3a58b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf  \n",
        "\n",
        "train_data = yf.download('AAPL','2014-01-01','2018-01-01')\n",
        "test_data = yf.download('AAPL','2018-01-01','2019-01-01')\n",
        "\n",
        "train_data.Close.plot()\n",
        "test_data.Close.plot()\n",
        "plt.legend ([\"Train\", \"Test\"])\n",
        "\n",
        "clear_output()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHDX0iqIorRw19IqiKK2c/wdjXYzY5QdymQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIIDgdQfcSK3",
        "colab_type": "text"
      },
      "source": [
        "As you can see our data is just a sequence of numbers ordered by time. \n",
        "\n",
        "In order to normalize our data we scale them between 0 and 1: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ge5thz254Kf",
        "colab_type": "code",
        "outputId": "5e3d22e6-5386-4205-86ff-bc1c57400ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler \n",
        "import numpy as np\n",
        "scaler = MinMaxScaler(feature_range = (0, 1)).fit(train_data.Close.values.reshape(-1, 1))\n",
        "\n",
        "train_scaled = scaler.transform(train_data.Close.values.reshape(-1, 1))\n",
        "test_scaled = scaler.transform(test_data.Close.values.reshape(-1, 1))  \n",
        "\n",
        "print (\"train_scaled.shape :\", train_scaled.shape)\n",
        "print (\"test_scaled.shape :\", test_scaled.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_scaled.shape : (1007, 1)\n",
            "test_scaled.shape : (251, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9M3_Ch_eRFx",
        "colab_type": "text"
      },
      "source": [
        "Now we create our input output pairs from our sequence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOs8KBcj6mpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_set = []  \n",
        "train_labels = []  \n",
        "for i in range(60, train_scaled.shape[0]):  \n",
        "    train_features_set.append(train_scaled[i-60:i, 0])\n",
        "    train_labels.append(train_scaled[i, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWmPH1aqfdzH",
        "colab_type": "text"
      },
      "source": [
        "We need to convert these lists to the numpy arrays before we can use them for training. Then in order to train LSTM on our data, we need to convert our data into the shape accepted by the LSTM. We need to convert our data into three-dimensional format. The first dimension is the number of records or rows in the dataset which is 1007 in our case. The second dimension is the number of time steps which is 60 while the last dimension is the number of indicators. Since we are only using one feature, i.e Closing price, the number of indicators will be one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVh8Igbrfw8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features_set, train_labels = np.array(train_features_set), np.array(train_labels)\n",
        "train_features_set = np.reshape(train_features_set, (train_features_set.shape[0], train_features_set.shape[1], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI8HP9NqiYzv",
        "colab_type": "text"
      },
      "source": [
        "Now let's create and train a simple model, our model has a stack of 4 lstm layers and a single output layer: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0UHr-Ef8pHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential  \n",
        "from keras.layers import Dense  \n",
        "from keras.layers import LSTM  \n",
        "from keras.layers import Dropout  \n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(train_features_set.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50, return_sequences=True))  \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50, return_sequences=True))  \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=50))  \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# we have a single output so this layer has 1 unit\n",
        "model.add(Dense(units = 1))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "model.fit(train_features_set, train_labels, epochs = 100, batch_size = 30)\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n8LuRDDqKkL",
        "colab_type": "text"
      },
      "source": [
        "Now we create our test features set similar to what we did with our training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Z0-AoCqZwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scaled = scaler.transform(test_data.Close.values.reshape(-1, 1))  \n",
        "\n",
        "test_features_set = []  \n",
        "for i in range(60, test_scaled.shape[0]):  \n",
        "    test_features_set.append(test_scaled[i-60:i, 0])\n",
        "    \n",
        "test_features_set = np.array(test_features_set)  \n",
        "test_features_set = np.reshape(test_features_set, (test_features_set.shape[0], test_features_set.shape[1], 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19NNvTHHqsdH",
        "colab_type": "text"
      },
      "source": [
        "And finaly let's see the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU6kKlck-UAL",
        "colab_type": "code",
        "outputId": "c8ae8f5f-7351-450b-acf8-4802f5d48076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "predictions = model.predict(test_features_set)\n",
        "predictions = scaler.inverse_transform(predictions)  \n",
        "  \n",
        "plt.plot(test_data.Close.values[60:], color='blue', label='Actual Apple Stock Price')  \n",
        "plt.plot(predictions , color='red', label='Predicted Apple Stock Price')  \n",
        "plt.title('Apple Stock Price Prediction on Test Set')    \n",
        "plt.ylabel('Apple Stock Price')  \n",
        "plt.legend()  \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxGQJwhhjTEz/D3a0Rb2r8GUDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ujgd8pNxre8D",
        "colab_type": "text"
      },
      "source": [
        "* At first sight, it seems the predictions are following  pretty well the actual trend. What do you think about the result? Are you willing to risk using this predictions in real world? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stxlKK1utSvU",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzlrh1NHtdnS",
        "colab_type": "text"
      },
      "source": [
        "### 2. Predicting The Direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHLBKZxXtpOP",
        "colab_type": "text"
      },
      "source": [
        "Previous model can be evaluated  just visually. We can evaluate a classification task using its special measures (such as f-score or accuracy) much better than a regression task. So in order to finaly decide whether the stocks market prediction is practical or not, we define this second task.\n",
        "\n",
        "Implement a model similar to previous one, this time the task is to predict whether the price is going to increase or decrease (output will be either 1 or 0 respectively) at time $T_i$ given prices at times $T_{i-60}$, ..., $T_{i-1}$ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a9C84JpuUiV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhvzyKn_ucd0",
        "colab_type": "text"
      },
      "source": [
        "Evaluate your trained model using the test set and report accuracy and f1-score your model archived: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW-MGFhgu4Lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaFxn4MR_4-4",
        "colab_type": "text"
      },
      "source": [
        "Do not worry if your results are not as promising as before :))\n",
        "* Comparing new results with previuos ones, why do you think this happens? Doesn't this task supposed to be an easier version of the previous one (if you are able to predict the actual price, you are already able to predict the direction of price trend)? Explain your toughts:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byIh3cwmBLt9",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ekiL12B6th",
        "colab_type": "text"
      },
      "source": [
        "* Intuitively explain what kind of properties do you think a time series need to have in order to be predictable? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAtpWmGFEIG4",
        "colab_type": "text"
      },
      "source": [
        "$\\color{red}{\\text{Write you answer here}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBjq-MvamPXO",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o4q5LiFiOx1",
        "colab_type": "text"
      },
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab downloaded file (`dl_asg05__xx__xx.zip`) and submit it via https://forms.gle/CwxHWdgkjB4xfduw9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRUf35av3ZP",
        "colab_type": "text"
      },
      "source": [
        "**Note:** We need your Github token to create a new repository  (if it doesn't exist previously) in order to store learned model data. Also Google Drive token enables us to download the current notebook and Create the submission. If you are interested, feel free to check our code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTytERc-vlaK",
        "colab_type": "text"
      },
      "source": [
        "## Create Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf1s5OvZVGHI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_05'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg05__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "course_url = 'https://iust-deep-learning.github.io/972/'\n",
        "\n",
        "! tar xf hub-linux-amd64-2.10.0.tgz\n",
        "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
        "! hub config --global hub.protocol https\n",
        "! hub config --global user.email \"$Your_Github_account_Email\"\n",
        "! hub config --global user.name \"$student_name\"\n",
        "! hub api -X GET /user\n",
        "! hub api -X GET /user > user_info.json\n",
        "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
        "\n",
        "user_info = json.load(open('user_info.json'))\n",
        "repos = json.load(open('repos.json'))\n",
        "repo_names = [r['name'] for r in repos]\n",
        "has_repository = repo_name in repo_names\n",
        "if not has_repository:\n",
        "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos homepage=\"%s\" > repo_info.json' % (repo_name, course_url))\n",
        "  repo_info = json.load(open('repo_info.json')) \n",
        "  repo_url = repo_info['clone_url']\n",
        "else:\n",
        "  username = user_info['login']\n",
        "  ! hub api -F homepage=\"$course_url\" -X PATCH /repos/$username/$repo_name\n",
        "  for r in repos:\n",
        "    if r['name'] == repo_name:\n",
        "      repo_url = r['clone_url']\n",
        "  \n",
        "stream = open(\"/root/.config/hub\", \"r\")\n",
        "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
        "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
        "\n",
        "! git clone \"$repo_url_with_token\"\n",
        "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
        "! cd \"$repo_name\" && git add -A\n",
        "! cd \"$repo_name\" && git commit -m \"Add assignment 05 results\"\n",
        "! cd \"$repo_name\" && git push -u origin master\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'repo_url': repo_url,\n",
        "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
        "    'datetime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9OFzaLtYu_",
        "colab_type": "text"
      },
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUzTlnX1nS8X",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJLRl0DL5aSO",
        "colab_type": "text"
      },
      "source": [
        "# References\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQPkvV83Kn0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Robbins, Herbert E. A Stochastic Approximation Method, 2007. https://doi.org/10.1214/aoms/1177729586.\n",
        "2. He, K., X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 77078, 2016. https://doi.org/10.1109/CVPR.2016.90.\n",
        "3. Wu, Yonghui, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, et al. Googles Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. ArXiv:1609.08144 [Cs], September 26, 2016. http://arxiv.org/abs/1609.08144.\n",
        "\n",
        "\n"
      ]
    }
  ]
}