{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-woU4Sodh6ND",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment #3\n",
        "\n",
        "\n",
        "Deep Learning / Spring 1398, Iran University of Science and Technology\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JWitIy1viFuD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Please pay attention to these notes:**\n",
        "\n",
        "<br/>\n",
        "- **Assignment Due: ** 1398/02/19 23:59\n",
        "- If you need any additional information, please review the assignment page on the course website.\n",
        "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
        "```\n",
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################\n",
        "```\n",
        "- We always recommend co-operation and discussion in groups for assignments. However, each student has to finish all the questions by him/herself. If our matching system identifies any sort of copying, you'll be responsible for consequences. So, please mention his/her name if you have a team-mate.\n",
        "- Students who audit this course should submit their assignments like other students to be qualified for attending the rest of the sessions.\n",
        "- Finding any sort of copying will zero down that assignment grade and also will be counted as two negative assignment for your final score.\n",
        "- When you are ready to submit, please follow the instructions at the end of this notebook.\n",
        "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
        "- You must run this notebook on Google Colab platform; there are some dependencies to Google Colab VM for some of the libraries.\n",
        "- **Before starting to work on the assignment please fill your name in the next section *AND Remember to RUN the cell.* **\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "Assignment Page: [https://iust-deep-learning.github.io/972/assignments/03_cnn](https://iust-deep-learning.github.io/972/assignments/03_cnn)\n",
        "\n",
        "Course Forum: [https://groups.google.com/forum/#!forum/dl972/](https://groups.google.com/forum/#!forum/dl972/)"
      ]
    },
    {
      "metadata": {
        "id": "0ejALNiDCWnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Vjwf6dWNCIQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fill your information here & run the cell"
      ]
    },
    {
      "metadata": {
        "id": "NeLkOPE6Qwr7",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Enter your information & \"RUN the cell!!\"\n",
        "student_id = 0 #@param {type:\"integer\"}\n",
        "student_name = \"\" #@param {type:\"string\"}\n",
        "Your_Github_account_Email = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "ASSIGNMENT_PATH = Path('asg03')\n",
        "ASSIGNMENT_PATH.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UMQvV4nljttN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Batch Normalization"
      ]
    },
    {
      "metadata": {
        "id": "JT_kxpjbNQUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training a deep neural network is a tricky process. Many techniques have already been proposed for more stable training and faster convergence. Most of these techniques either change the model architecture or improve the training algorithm. Batch normalization belongs to the former group. The method was introduced in 2015 and achieved state-of-the-art in ImageNet,  a well-known image classification benchmark."
      ]
    },
    {
      "metadata": {
        "id": "vDcK52jRj_mD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 Definition "
      ]
    },
    {
      "metadata": {
        "id": "owKLAOJVj73I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the first assignment, you might remember that we generally normalize the inputs of a neural network to speed up the convergence. So if the \"normalization\" works, why not try it on the activation values? How can we improve training by normalizing the values of intermediate layers?\n",
        "\n",
        "Here is an intermediate layer $l$ in some neural network:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/01_intermediate_layer.jpg\" width=\"500\"/></p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nKOwOymYSVK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The general idea is to train layer $l$ faster by normalizing its input. By the layer $l$ we simply mean weigth matrices $W^{l}$, $b^{l}$ and by the input we mean previous layer's activations $a^{l-1}$. For the sake of simplicity, let us change our notation. Instead of normalizing the input of layer $l$, we would like to normalize the output so that the next layers will receive normalized values from our layer. It has the same effect, but it will make the equations much cleaner.\n",
        "\n",
        "In practice, we do not normalize the output (the activations). Instead, we do the normalization on the weighted sum of inputs $Z^{l}$ just before applying the activation function ($Z^l = xW^l+b^l$).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D_xmxSzHkqrT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 The formula"
      ]
    },
    {
      "metadata": {
        "id": "PjFpIC3HgxrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Assume we want some variable $x$ to have normalized values, the only way to do that is to collect all of its values and calculate the mean and variance in order to create a normalized version of $x$. This is a fairly reasonable solution, and we use it to normalize the neural network's input. Now imagine the goal is to normalize some intermediate values in a deep neural network; Collecting values of some intermediate point in a neural network is almost impossible since the training algorithm can change them entirely. To overcome this issue, we can collect them over a mini-batch.  It will give us an estimated version of the mean and variance. This is why it is called Batch Normalization. Here is the detailed algorithm:\n",
        "\n",
        "Given values of $x$ over a mini-batch $\\mathcal{B} = \\{x_1, .., x_m\\}$ :\n",
        "\n",
        "$$\n",
        "\\mu _\\mathcal{B} = \\frac{1}{m} \\sum^{m}_{i=1} x_i  \\ \\ \\ \\ \\ \\text{(mini-batch mean)}\n",
        "\\\\ \n",
        "\\sigma^2 _\\mathcal{B} = \\frac{1}{m} \\sum^{m}_{i=1} (x_i-\\mu _\\mathcal{B}) \n",
        "\\ \\ \\ \\ \\ \\text{(mini-batch variance)}\n",
        "\\\\\n",
        "x^{norm}_i = \\frac{x_i - \\mu _\\mathcal{B}}{\\sqrt{\\sigma^2 _\\mathcal{B} + \\epsilon}} \\ \\ \\ \\ \\ \\text{(normalize)} \n",
        "\\\\ \n",
        "\\hat{x}_i =\\gamma x^{norm}_i+\\beta  \\ \\ \\ \\ \\ \\text{(scale and shift)} \n",
        "\\\\\n",
        "\\mathrm{BN(\\mathcal{B}, \\gamma, \\beta}) = \\{\\hat{x}_1, ..., \\hat{x}_m\\}\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "72SOE7UfxmEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Notes:**\n",
        "1. All of the notations above are non-vector.\n",
        "2. $\\gamma$ and $\\beta$ are learnable parameters.\n",
        "3. $\\epsilon$ is just a small number, and we use it for numerical stability.\n",
        "4. $\\mathrm{BN}$ function calculates its output based on a batch of values. Consequently, we'll have different $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ for each mini-batch during the training process. We will reference this property in the next sections.\n",
        "5. $x^{norm}_i$ is actually the normalized version of $x_i$ which has mean 0 and variance 1. However, hidden units in neural networks have different distributions, and we don't really want them to all have the same distribution, So instead, we just scale and shift $x^{norm}_i$ with two variables $\\gamma$, $\\beta$. \n",
        "6. Another reason for the extra \"scale & shift\" step is that if we choose $\\gamma = \\sqrt{\\sigma^2_\\mathcal{B} + \\epsilon}$ and $\\beta = \\mu_\\mathcal{B}$ then $\\hat{x}_i$ will become $x_i$, So the optimizer can easily remove the batch normalization if it is sufficient for proper training.\n",
        "\n",
        "One difference between normalizing a neural network's inputs and Batch Normalization is that the latter does not force values to have mean 0 and variance 1. Explain it with at least one reason, why we might not want the hidden units to be forced to have mean 0 and variance 1."
      ]
    },
    {
      "metadata": {
        "id": "rrTaUoPMLhyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "8few4PaaXf9t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Batch normalization at test time\n",
        "\n",
        "As we said, We will have multiple $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ since they are calculated individually for each mini-batch. So What should we do for the test time? In fact, the idea is quite simple; We can just calculate a moving average of $\\mu_\\mathcal{B}$ and $\\sigma^2_\\mathcal{B}$ to use at test time. Deep learning frameworks such as Tensorflow are using this algorithm in their default bach normalization implementations."
      ]
    },
    {
      "metadata": {
        "id": "wwyIr1ChkvzY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3 Applying the batch-norm on layers"
      ]
    },
    {
      "metadata": {
        "id": "Wu85xhmM1xFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Batch Normalization (or simply batch-norm) doesn't know anything about the concept of layers and vectors. we have to integrate it manually in our layers. For a given d-dimensional vector of logits $Z = (z^{(1)},..., z^{(d)})$, the batch-normalized version is \n",
        "\n",
        "$$\n",
        "Z = (\\ \\mathrm{BN}(\\mathcal{B}\\{z^{(1)}\\}, \\gamma^{(1)}, \\beta^{(1)}),..., \\mathrm{BN}(\\mathcal{B}\\{z^{(d)}\\}, \\gamma^{(d)}, \\beta^{(d)})\\ )\n",
        "$$\n",
        "\n",
        "As you might have noticed, we need a batch for each $Z$'s element in the latter version. In other words, we need a batch of $Z$. Fortunately, this is good news for us since we build our neural networks entirely based on batches.\n",
        "\n",
        "Write the vectorized version of batch-norm equations and specify the dimensions.\n",
        "\n",
        "For any given layer $l$ with $n$ hidden units and batch size $b$:\n",
        "\n",
        "$$\n",
        "z = xW + B\\ \\ \\ \\  z \\in \\mathbb{R} ^ {b \\times n}, W \\in \\mathbb{R} ^ {m \\times n}, B \\in \\mathbb{R} ^ {b \\times n}, x \\in \\mathbb{R} ^ {b \\times m}\n",
        "$$\n"
      ]
    },
    {
      "metadata": {
        "id": "A-J18TTk_Uh1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\mu = \\color{red}{\\text{Write your answer here}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2 = \\color{red}{\\text{Write your answer here}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "z^{norm} = \\color{red}{\\text{Write your answer here}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{z} = \\gamma \\odot z^{norm} + \\beta  \\ \\ \\ \\ \\ \\ (\\odot \\text{ is an element-wise dot product} )\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "eZhWd3X7BR7D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Write dimensions for $\\mu$, $\\sigma^2$, $z^{norm}$, $\\gamma$, $\\beta$, and any intermediate variable you've defined:"
      ]
    },
    {
      "metadata": {
        "id": "zgEe2U1bB13k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "UmrjugAKYs0p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imagine a simple neural network with l hidden layers. We want to apply the batch-norm on all layers. Here is how it would look ($\\mathcal{X}^{b} $ is an input batch):\n",
        "\n",
        "$$\n",
        "\\mathcal{X}^{b}\\stackrel{W^{[1]}, B^{[1]}}{\\longrightarrow}Z^{[1]} \\stackrel{\\gamma^{[1]}, \\beta^{[1]}}{\\longrightarrow}\\hat{Z}^{[1]} \\longrightarrow a^{[1]} = func^{[1]}(\\hat{Z}^{[1]})\\stackrel{W^{[2]}, B^{[2]}}{\\longrightarrow} ...\n",
        "$$\n",
        "\n",
        "Also, the parameters for that neural network would be:\n",
        "$$\n",
        "W^{[1]}, B^{[1]} \\ \\  \\ \\ W^{[2]}, B^{[2]}  \\ \\ ... \\ \\ W^{[l]}, B^{[l]} \n",
        "\\\\\n",
        "\\gamma^{[1]}, \\beta^{[1]} \\ \\ \\ \\  \\  \\ \\ \\gamma^{[2]}, \\beta^{[2]}  \\ \\ \\ \\  ... \\ \\ \\ \\ \\gamma^{[l]}, \\beta^{[l]} \n",
        "$$\n",
        "\n",
        "$B^{[i]}$ is the bias term in our neural, but with incorporating the batch-norm and introduction of new variables, $B^{[i]}$ will become unnecessary. Prove that why."
      ]
    },
    {
      "metadata": {
        "id": "Tqv6BuLfirk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "T0PrEPOdlE9d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Why does it work?"
      ]
    },
    {
      "metadata": {
        "id": "AVqGS4J6lJlf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imagine a super simple neural network:\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/02_simple_nn.png\" width=\"300\"/>\n",
        "<br>\n",
        "  [[source](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)]\n",
        "</p>\n",
        "\n",
        "</br>\n",
        "During the training, Our optimizer calculates the gradients with respect to weights. Take layer **a** as an example; Optimizer calucates  $\\frac{\\partial L}{\\partial a}$ and then it updates the weights for this layer. Unfortunately,  it means that weight update for Layer **a** only depends on the sensitivity of loss function to that weight. However, changing weights of initial layers can completely effect the statistics of any futher layer.\n",
        "\n",
        "With the presence of Batch Normalization, our optimizer package can now adjust two parameters $\\gamma$, $\\beta$ to change statistics of any layer, rather than entire weight matrix. It makes the training of any layer independent and also introduces some checkpointing mechanism.\n",
        "\n",
        "Besides, recent findings show that batch normalization smoothes the landscape/surface of the loss function, effectively making the optimization performance less dependant on the initial state.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/03_error_surface.jpg\"  width=\"700\"/>\n",
        "<br/>\n",
        "  source: [2]\n",
        "  </p>"
      ]
    },
    {
      "metadata": {
        "id": "VZlJ4Hic2aoN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5 Batch Normalization in action\n"
      ]
    },
    {
      "metadata": {
        "id": "cSqjUrD02hiL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's create a layer to use batch normalization easier."
      ]
    },
    {
      "metadata": {
        "id": "YwY8vwus2xQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Layer, Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gt5mzXXX2s5V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BatchNormalizedLayer(Layer):\n",
        "  def __init__(self, layer, axis=-1, activation=None, **kwargs):\n",
        "    \"\"\"Runs batch normalization on layer instance and applies the activation function\n",
        "    \n",
        "    Args:\n",
        "      layer(layers.Layer): A layer to normalize its output\n",
        "      axis(int): the axis that should be normalized (typically the features axis).\n",
        "      activation(str): Activation function to use\n",
        "    \"\"\"\n",
        "    super(BatchNormalizedLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    self.layer = layer\n",
        "    self.activation = activation\n",
        "    self.axis = axis\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    \"\"\"Runs the layer\n",
        "    \n",
        "    Args:\n",
        "      inputs: The layer's input\n",
        "      \n",
        "    hint: keras.layers.normalization.BatchNormalization and layers.Activation might be useful for you\n",
        "    \"\"\"\n",
        "    ########################################\n",
        "    #     Put your implementation here     #\n",
        "    ########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cc0wyxlb7JwJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "bnl = BatchNormalizedLayer(Dense(5), activation='relo')\n",
        "x = K.constant(2.5 * np.random.randn(10, 4) + 3)\n",
        "\n",
        "assert K.eval(bnl(x)).shape == (10, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-Tlpqm-_dyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.5.1 CNN"
      ]
    },
    {
      "metadata": {
        "id": "ZPLoHHW1_qdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have our special layer. So, let's use it in a real neural network. We want to improve the baseline using the Batch Normalization layer. Our desired task is CIFAR10 image  classification. \n",
        "\n",
        "First, let's load the dataset:"
      ]
    },
    {
      "metadata": {
        "id": "wm79YEyIAmKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Visualizing CIFAR 10\n",
        "fig, axes1 = plt.subplots(2,5,figsize=(10,4))\n",
        "for j in range(2):\n",
        "  for k in range(5):\n",
        "    i = np.random.choice(range(len(x_train)))\n",
        "    axes1[j][k].set_axis_off()\n",
        "    axes1[j][k].imshow(x_train[i:i+1][0])\n",
        "        \n",
        "# Normalize\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UBb__seXIyRN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_loss_and_acc(history):\n",
        "  history_dict = history.history\n",
        "  loss_values = history_dict['loss']\n",
        "  val_loss_values = history_dict['val_loss']\n",
        "  acc = history_dict['acc']\n",
        "\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  f = plt.figure(figsize=(10,3))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "  plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  acc_values = history_dict['acc']\n",
        "  val_acc = history_dict['val_acc']\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qlKS6lsIzLX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Baseline"
      ]
    },
    {
      "metadata": {
        "id": "CzzXJ2M6GXn-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the baseline model"
      ]
    },
    {
      "metadata": {
        "id": "ZnXcRHpuDcvS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_baseline_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=x_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TQ2uoSmGlvP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the baseline"
      ]
    },
    {
      "metadata": {
        "id": "2Q45b4cVF36l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "488nX4FSDzs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the baseline model\n",
        "baseline = get_baseline_model()\n",
        "\n",
        "# Train model\n",
        "bs_history = baseline.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AJgQ9yxpGp8P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize the training and evaluate the model"
      ]
    },
    {
      "metadata": {
        "id": "VeVuiJmHGUw7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(bs_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8aMc7UWPICG9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = baseline.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ufqdGaLrI4oZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Improved"
      ]
    },
    {
      "metadata": {
        "id": "dm8Yu20HHBXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now update the baseline to create an enhanced model only by using `BatchNormalizedLayer` "
      ]
    },
    {
      "metadata": {
        "id": "aD6NqqZ3Hbeg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_improved_model():\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  ########################################\n",
        "  #     Put your implementation here     #\n",
        "  ########################################\n",
        "  \n",
        "  # model.add(Conv2D(32, (3, 3), padding='same', input_shape=x_train.shape[1:]))\n",
        "  # model.add(Activation('relu'))\n",
        "  # model.add(Conv2D(32, (3, 3)))\n",
        "  # model.add(Activation('relu'))\n",
        "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  # model.add(Activation('relu'))\n",
        "  # model.add(Conv2D(64, (3, 3)))\n",
        "  # model.add(Activation('relu'))\n",
        "  # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  # model.add(Dropout(0.25))\n",
        "\n",
        "  # model.add(Flatten())\n",
        "  # model.add(Dense(512))\n",
        "  # model.add(Activation('relu'))\n",
        "  # model.add(Dropout(0.5))\n",
        "  # model.add(Dense(num_classes))\n",
        "  # model.add(Activation('softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYmlozp3I8Ts",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train and evaluate"
      ]
    },
    {
      "metadata": {
        "id": "mLNW4dnsH9NF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the baseline model\n",
        "impv_model = get_improved_model()\n",
        "\n",
        "# Train model\n",
        "impv_history = impv_model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fsz97z1Q-eoA"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize the training and evaluate the model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "30JMMBQyIb49",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_loss_and_acc(impv_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZoqlKrUAIb4_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Score trained model.\n",
        "scores = impv_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZS70rhY9JCWb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remember to run this cell after each time you update the model, \n",
        "# this is a deliverable item of your assignemnt\n",
        "impv_model.save(str(ASSIGNMENT_PATH / 'cifar_impv.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-vfDkCCycqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Question:** Compare your model to the baseline. What are the diffrences? Does batch normalization work?"
      ]
    },
    {
      "metadata": {
        "id": "VjOOqiDDyf4p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "zeDLckyY_Yuq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Denoising\n"
      ]
    },
    {
      "metadata": {
        "id": "FBTrW5p70CZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this part, we are going to implement a simple denoising network. The purpose of this assignment is to help you become familiar with autoencoders and Conv2DTranspose (known as deconvolution layer) and UpSampling2D layers in keras.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1M7WxbCP357r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.1 Problem Definition"
      ]
    },
    {
      "metadata": {
        "id": "q1hqIkLa4lS6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have a noisy version of mnist data. The goal in this assignment is to remove the noise using an autoencoder neural network."
      ]
    },
    {
      "metadata": {
        "id": "hb9Gmx8Ta-80",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Autoencoder:\n",
        "An autoencoder neural network has the following architecture:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/06_autoencoder_structure.png\" width=\"500\"/>  \n",
        "  [[source](https://en.wikipedia.org/wiki/Autoencoder)]\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "The encoder part usually takes the input (X) and makes a more dense representation of it (z) and the decoder part tries to make the output (X') from the coded representation. Since the network itself learns how to encode the input data into some meaningful feature representation,  we call this network an \"autoencoder\"'.\n"
      ]
    },
    {
      "metadata": {
        "id": "aCc4VG6de_A3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Denoising Using Autoencoders:\n",
        "\n",
        "Autoencoders have a wide varity of usages. One usage is to reduce the noise in an input image.\n",
        "Suppose we train an autoencoder by feeding some noisy images as inputs and having the noise-free version of those images as desired outputs. This way the encoder part learns to extract the most important features of the input image and the decoder part learns to reconstruct noise-free images from extracted features. "
      ]
    },
    {
      "metadata": {
        "id": "TjkLZ78-hyPV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.2 Solving The Problem"
      ]
    },
    {
      "metadata": {
        "id": "pPyBifm4k6A7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Preparing Mnist Data:"
      ]
    },
    {
      "metadata": {
        "id": "75eIwg2LiEyD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets prepare our data"
      ]
    },
    {
      "metadata": {
        "id": "6DmAp9_diPTs",
        "colab_type": "code",
        "outputId": "465993a8-7338-4504-99c8-3eba75584344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# We don't really need the classification lables, just images\n",
        "(real_images_train, _ ), (real_images_test, _ ) = mnist.load_data()\n",
        "# Normalizing values to be between 0 and 1\n",
        "real_images_train = real_images_train / 255.0\n",
        "real_images_test = real_images_test / 255.0\n",
        "\n",
        "plt.imshow (real_images_test[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GCdPn8WIjXPh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Lets add some noise to our train and test sets"
      ]
    },
    {
      "metadata": {
        "id": "nAjk6nZyjWIa",
        "colab_type": "code",
        "outputId": "13378c7b-d790-411b-e9c6-6fc7cc30896f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "noise = np.random.random((60000, 28, 28))\n",
        "noisy_images_train = np.copy(real_images_train)\n",
        "noisy_images_train[np.where(noise>0.9)] = 1.0\n",
        "noisy_images_train[np.where(noise<0.1)] = 0.0\n",
        "\n",
        "noise = np.random.random((10000, 28, 28))\n",
        "noisy_images_test = np.copy(real_images_test)\n",
        "noisy_images_test[np.where(noise>0.9)] = 1.0\n",
        "noisy_images_test[np.where(noise<0.1)] = 0.0\n",
        "\n",
        "plt.imshow(noisy_images_test[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "pxfXy7Y6kARy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And finally reshape our data sets in order to be compatible with keras layers"
      ]
    },
    {
      "metadata": {
        "id": "e2gqrC4gkQGY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_train = noisy_images_train.reshape((60000, 28, 28, 1))\n",
        "output_train = real_images_train.reshape((60000, 28, 28, 1))\n",
        "\n",
        "input_test = noisy_images_test.reshape((10000, 28, 28, 1))\n",
        "output_test = real_images_test.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4v9A5de7kq1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creating the model:"
      ]
    },
    {
      "metadata": {
        "id": "rHiIK3KJlGFG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We build the encoder part using some convolution and pooling layers:"
      ]
    },
    {
      "metadata": {
        "id": "N__RutTwnloq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "def encoder (input_layer):\n",
        "  x = Conv2D(16, (3, 3), activation='relu')(input_layer)\n",
        "  x = MaxPooling2D((2, 2))(x)\n",
        "  x = Conv2D(8, (2, 2), activation='relu')(x)\n",
        "  x = MaxPooling2D((2, 2))(x)\n",
        "  encoded = Conv2D(8, (3, 3), activation='relu')(x)\n",
        "  return encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o2Irpfihn-i8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you know, passing an input of size (x, y) to a convolution layer with filter size (h, h), stride equal to one and no padding, results in a  (x+1-h, y+1-h) sized output feature map, and also we know each polling layer with filter size (r, r) will reduce both the heigh and width of the input by a factor of *r*.\n",
        "So if we assume that input size is 28 by 28 (mnist image size), the output of this encoder will be 8 feature maps and each of theme is in size of 4 by 4 (the output shape will be  (4, 4, 8) )\n",
        "\n",
        "Now we want to build the decoder part using a stack of *Conv2DTranspose* and *UpSampling2D* layers.\n",
        "\n",
        "So **why do we need upsampling in the first place?**  When we use neural networks to generate images, it usually involves up-sampling from low resolution to high resolution, as in this example, we need to reconstruct 28x28 pictures from extracted featur maps.\n",
        "\n",
        "There are various methods to conduct up-sampling operation, the first and the most simple method is to use  upsampling layers. An upsampling layer simply increases the size of input image using known interpolation methods (such as bi-linear or bi-cubic).\n",
        "\n",
        "Another way is using a convolution transpose layer (or known as deconvolution layer). In a normal convolution operation, we calculate the sum of the element-wise multiplication between the input matrix and kernel matrix. More concretely, the $h* h$ kernel is used to connect the $h^2$ values in the input matrix to $1$ value in the output matrix. A convolution operation forms a **many-to-one** relationship. On the other hand, a deconvolution operation is a  **one-to-many** operaion.\n",
        "\n",
        "As an example by using a convolution layer with kernel size (3, 3) on an image with size (4, 4), we get an image of size (2, 2). We can reverse this process and simply use a convolution transpose layer with kernel size (3, 3) on top of an image with size (2, 2) to get an upsampled version by size (4, 4). See the following picture for a better understanding:\n",
        "\n",
        "<p align=\"center\"><img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/07_deconv.jpg\n",
        "\" width=\"800\"/></p>\n",
        "\n",
        "<br>\n",
        "\n",
        "Using convolution transpose layer can have side effects, one of them is known as **checkerboard artifacts**. Read about this side effect and explain why do you think it happens?\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "EMUNDRefz3nC"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "6hDL4fSiz7cG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, what is the difference between upsampling and convolution transpose layers? The answer is that the upsampling layer has no parameters to learn and simply does some mathematical operations on the input image in order to make higher resolution images, but a convolution transpose layer uses a trainable kernel to upsample the input.\n",
        "\n",
        "Now, using these layers we can build our decoder. Read the full documentation of these two layers, and use the right size for each layer in order to make a 28 by 28 output:\n"
      ]
    },
    {
      "metadata": {
        "id": "W8RijzHTwcpA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2DTranspose, UpSampling2D\n",
        "\n",
        "def decoder (input_layer):\n",
        "  \n",
        "  ##########################################\n",
        "  # fill the ? marks with appropriate size #\n",
        "  ##########################################\n",
        "  \n",
        "  x = Conv2DTranspose(8, (?, ?), activation='relu')(input_layer) \n",
        "  x = UpSampling2D((?, ?))(x)\n",
        "  x = Conv2DTranspose(8, (?, ?), activation='relu')(x)\n",
        "  x = UpSampling2D((?, ?))(x)\n",
        "  decoded = Conv2DTranspose(1, (?, ?), activation='relu')(x)\n",
        "  return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqneegofxROA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Putting parts together:"
      ]
    },
    {
      "metadata": {
        "id": "I7vKupC8xW3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "input_layer = Input(shape=(28, 28, 1))\n",
        "features = encoder (input_layer)\n",
        "output_layer = decoder (features)\n",
        "\n",
        "autoencoder = Model(input_layer, output_layer)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UiWvoa4Py3Bg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Training The Model####"
      ]
    },
    {
      "metadata": {
        "id": "F7bUrUU-yYrX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.fit(input_train, output_train,\n",
        "                epochs=10,\n",
        "                batch_size=128,\n",
        "                shuffle=True,\n",
        "                validation_data=(input_test[:500], output_test[:500]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58_QsHtHys3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Testing The Result "
      ]
    },
    {
      "metadata": {
        "id": "3kiLyGnry_O0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoded_imgs = autoencoder.predict(input_test)\n",
        "\n",
        "n_to_visualize = 20\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n_to_visualize+1):\n",
        "    # display original\n",
        "    ax = plt.subplot(2, n_to_visualize, i)\n",
        "    plt.imshow(input_test[i].reshape(28, 28))\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(2, n_to_visualize, i + n_to_visualize)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cZHYqTt6duYj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't forget to run this cell.\n",
        "# this is a deliverable item of your assignemnt\n",
        "autoencoder.save(str(ASSIGNMENT_PATH / 'denoising_autoenc.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u75b8GGLHl-S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. Semantic segmentation"
      ]
    },
    {
      "metadata": {
        "id": "EDgOMPZh14Ni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/04_img_seg.jpg\"  width=500 />\n",
        "\n",
        "[source](https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef)\n",
        "\n",
        "Semantic segmentation is one of the high-level computer vision tasks that helps the machins to understand a scene. In the semantic segmentation task, we aim to label an image at pixel level, so that we can detect the boundaries of an object in the picture. This task has many applications in the area of self-driving vehicles, human-computer interaction, virtual reality, etc.\n",
        "\n",
        "\n",
        "For example, if you have to explainthe up right image in the following picture, how you are going to do it? Is the following sentence is a good answer?\n",
        "\n",
        "**\"Two men riding on a bike in front of a building on the road. And there is a car.\"**\n",
        "\n",
        "<img src=\"https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/05_img_seg_02.png\" width=500>\n",
        "\n",
        "[source](http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf)\n",
        "\n",
        "How a machine can generate such an information about the image? To generate such an explanation about a scene a machine should be able to detect different objects and their boundaries in the image.\n"
      ]
    },
    {
      "metadata": {
        "id": "o6RvhwUvAAEh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**How to do semantic segmentation?**\n",
        "\n",
        "Semantic segmentation divides into to steps:\n",
        "\n",
        "\n",
        "1.   encode the input image to extract features\n",
        "2.   decode the extracted features into a higher resolousion picture\n",
        "\n",
        "Usualy for the first step, we use a pretrained network to extract features, and then, use the output of that network to generate the higher resolousion image.\n",
        "\n",
        "The most common loss function for this task is a pixel-wise cross entropy loss. This loss examines each pixel individually.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In this assignment, we want to implement a convolutional deep neural network that detects horses and dogs in an image and shows us boundaries of object in the picture. \n",
        "\n",
        "we are going to use the HorseSeg and DogSeg datasets. According to the [Alexander Kolesnikov's web page](http://pub.ist.ac.at/~akolesnikov/HDSeg/): \n",
        " \n",
        "     \"HorseSeg and DogSeg are two semantic image segmentation datasets with 25,679 and 158,984 images respectively. The images for these datasets were collected from the ImageNet dataset and PASCAL VOC12. Images from PASCAL VOC12 have manually created annotations. Images from the ImageNet dataset dataset have weak supervision which is per-image labels and approximatelly one quater of all images also have object bounding boxes.\"\n",
        "     \n",
        "First of all, you should choose one of the datasets. You can either choose the HorseSeg or the DogSeg. In the following box write down the dataset you have chosen."
      ]
    },
    {
      "metadata": {
        "id": "nnzqMSOhe1LE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\color{red}{\\text{Write your answer here}}$"
      ]
    },
    {
      "metadata": {
        "id": "ZOwOInjYd_al",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "After that, you need to download the training set and annotations. \n",
        "1.  You can download the annotations from [here](http://pub.ist.ac.at/~akolesnikov/HDSeg/HDSeg.tar).\n",
        "2.   There is a script in the dowloaded file which you should run to download the dataset. Run it as we did in the following code box.\n",
        "3.   When you run the code, it will ask you for your image-net username and access key. You can use your academic email to obtain a username and access key to the image-net at the [image-net.org](http://www.image-net.org/)\n",
        "4.   If you could not obtain your access key you can use the other file which we have uploaded on the course webpage."
      ]
    },
    {
      "metadata": {
        "id": "lTHyilu31Ojh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the dataset and its labels\n",
        "! wget http://pub.ist.ac.at/~akolesnikov/HDSeg/HDSeg.tar\n",
        "\n",
        "! wget https://iust-deep-learning.github.io/972/static_files/assignments/asg03_assets/download_imagenet.sh\n",
        "! bash download_imagenet.sh HorseSeg\n",
        "! bash download_imagenet.sh DogSeg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mGTF99oCUAxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the following code to load the data into the runtime machine. Shape of each images in this dataset is different, but we convert all of them to a 256x256."
      ]
    },
    {
      "metadata": {
        "id": "o8fKJqRhUai1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "\n",
        "def retrieve_file_names(dataset_name):\n",
        "    if dataset_name not in ['HorseSeg', 'DogSeg']:\n",
        "      print('dataset name is not valid')\n",
        "      return None\n",
        "    path = 'images/%s' % dataset_name\n",
        "    files_path = []\n",
        "    dirs= os.listdir(path)\n",
        "    for d in dirs:\n",
        "      inner_path = path + '/' + d\n",
        "      files = os.listdir(inner_path)\n",
        "      for file in files:\n",
        "        files_path.append(inner_path + '/' + file)\n",
        "    return files_path\n",
        "\n",
        "def load_data(dataset_name):\n",
        "    if dataset_name not in ['HorseSeg', 'DogSeg']:\n",
        "      print('dataset name is not valid')\n",
        "      return None, None\n",
        "    files_path = retrieve_file_names(dataset_name)\n",
        "    print(files_path)\n",
        "    dataset = []\n",
        "    dataset_y = []\n",
        "    for path in files_path:\n",
        "      img = image.load_img(path, target_size=(256, 256))\n",
        "      x = image.img_to_array(img)\n",
        "      dataset.append(x)\n",
        "      img_path = path\n",
        "      img_path = img_path.replace(\"images\", \"annotations\")\n",
        "      img = image.load_img(img_path, target_size=(256, 256))\n",
        "      x = image.img_to_array(img)\n",
        "      dataset_y.append(x)\n",
        "    dataset = np.array(dataset)\n",
        "    dataset_y = np.array(dataset_y)\n",
        "\n",
        "    nb_sample = dataset.shape[0]\n",
        "    split = int(nb_sample * 0.75)\n",
        "    training, training_y, test, test_y = dataset[:split,:], dataset_y[:split,:], dataset[split:,:], dataset_y[split:,:]\n",
        "    return training, test\n",
        "\n",
        "training, training_y, test, test_y = load_data('HorseSeg')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDceV3q_1Utn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Use a pre-trained network**\n",
        "\n",
        "Keras provides a set of pre-trained networks, use this [link](https://keras.io/applications/) and take a look at them. In this assignment, we want to use VGG19 network for segmentation.\n",
        "\n",
        "As you see in the following code box, we imported the ResNet50 network."
      ]
    },
    {
      "metadata": {
        "id": "X0tndPNK1Q8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "\n",
        "model = ResNet50(weights='imagenet', include_top=False)\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ctLjYBfWrkT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the following code box, implement a code that loads the 'block5_conv2' layer of the VGG19 network , and in the next one load the 'fc2' layer of the network."
      ]
    },
    {
      "metadata": {
        "id": "4SGXG-MCaZFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6ZKEVRgaYvD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m1YaHEeF1RYO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use a stack of the [deconvolution layers](https://keras.io/layers/convolutional/#conv2dtranspose) and other types of layers that you have learned to implement the encoder part of this network. \n",
        "\n",
        "1.   In the following code box, implement your encoder.\n",
        "2.   In the next two code boxes, attach the encoder to the decoders, and then, train the networks."
      ]
    },
    {
      "metadata": {
        "id": "UBp6JaqqdaXp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6wRcRg_daIp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ywr5SxFxcksS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fhmVrRpDdiFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, it is time to evaluate your network. Write the testing process in the following box. You should use the testset that you have loaded before to evaluate your network.\n",
        "\n",
        "**Note:** you should report accuracy and loss for training set, validation set, and test set. You should also demonstrate 10 input image and the regions that were predicted to be horse or dog."
      ]
    },
    {
      "metadata": {
        "id": "2xncal2pgVYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RQ_p6Nivggsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Optional:**\n",
        "\n",
        "This part is optional and you can ignore it if you are not interested in it. But, if you are interested, you can implement one more network and get extra score.\n",
        "\n",
        "In this part you should implement both parts of the network by yourself, without using any pretrained network. Note that this network should be trained on both of the datasets. The output of this network for horses and dogs must be different. For example, it should mark the regions that it sees a dog with blue, the regions that it sees a horse with red, and other regions with black.\n",
        "\n",
        "**Note:** copying a code from github will result a negative  score for this part."
      ]
    },
    {
      "metadata": {
        "id": "WmumP4GIk_RB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#     Put your implementation here     #\n",
        "########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pBjq-MvamPXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submission"
      ]
    },
    {
      "metadata": {
        "id": "7o4q5LiFiOx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Congratulations! You finished the assignment & you're ready to submit your work. Please follow the instruction:\n",
        "\n",
        "1. Check and review your answers. Make sure all of the cell outputs are what you want. \n",
        "2. Select File > Save.\n",
        "3. Run **Create Submission** cell, It may take several minutes and it may ask you for your credential.\n",
        "4. Run **Download Submission** cell to obtain your submission as a zip file.\n",
        "5. Grab downloaded file (`dl_asg03__xx__xx.zip`) and submit it via https://forms.gle/WEAHLkhvUAHTyYux6."
      ]
    },
    {
      "metadata": {
        "id": "iWRUf35av3ZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note: ** We need your Github token to create a new repository  (if it doesn't exist previously) in order to store learned model data. Also Google Drive token enables us to download the current notebook and Create the submission. If you are interested, feel free to check our code."
      ]
    },
    {
      "metadata": {
        "id": "cTytERc-vlaK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create Submission (Run the cell)"
      ]
    },
    {
      "metadata": {
        "id": "kf1s5OvZVGHI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "! pip install -U --quiet PyDrive > /dev/null\n",
        "! wget -q https://github.com/github/hub/releases/download/v2.10.0/hub-linux-amd64-2.10.0.tgz \n",
        "  \n",
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import Javascript\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "asg_name = 'assignment_03'\n",
        "script_save = '''\n",
        "require([\"base/js/namespace\"],function(Jupyter) {\n",
        "    Jupyter.notebook.save_checkpoint();\n",
        "});\n",
        "'''\n",
        "repo_name = 'iust-deep-learning-assignments'\n",
        "submission_file_name = 'dl_asg03__%s__%s.zip'%(student_id, student_name.lower().replace(' ',  '_'))\n",
        "course_url = 'https://iust-deep-learning.github.io/972/'\n",
        "\n",
        "! tar xf hub-linux-amd64-2.10.0.tgz\n",
        "! cd hub-linux-amd64-2.10.0/ && chmod a+x install && ./install\n",
        "! hub config --global hub.protocol https\n",
        "! hub config --global user.email \"$Your_Github_account_Email\"\n",
        "! hub config --global user.name \"$student_name\"\n",
        "! hub api -X GET /user\n",
        "! hub api -X GET /user > user_info.json\n",
        "! hub api -F affiliation=owner -X GET /user/repos > repos.json\n",
        "\n",
        "user_info = json.load(open('user_info.json'))\n",
        "repos = json.load(open('repos.json'))\n",
        "repo_names = [r['name'] for r in repos]\n",
        "has_repository = repo_name in repo_names\n",
        "if not has_repository:\n",
        "  get_ipython().system_raw('! hub api -X POST -F name=%s /user/repos homepage=\"%s\" > repo_info.json' % (repo_name, course_url))\n",
        "  repo_info = json.load(open('repo_info.json')) \n",
        "  repo_url = repo_info['clone_url']\n",
        "else:\n",
        "  username = user_info['login']\n",
        "  ! hub api -F homepage=\"$course_url\" -X PATCH /repos/$username/$repo_name\n",
        "  for r in repos:\n",
        "    if r['name'] == repo_name:\n",
        "      repo_url = r['clone_url']\n",
        "  \n",
        "stream = open(\"/root/.config/hub\", \"r\")\n",
        "token = list(yaml.load_all(stream))[0]['github.com'][0]['oauth_token']\n",
        "repo_url_with_token = 'https://'+token+\"@\" +repo_url.split('https://')[1]\n",
        "\n",
        "! git clone \"$repo_url_with_token\"\n",
        "! cp -r \"$ASSIGNMENT_PATH\" \"$repo_name\"/\n",
        "! cd \"$repo_name\" && git add -A\n",
        "! cd \"$repo_name\" && git commit -m \"Add assignment 02 results\"\n",
        "! cd \"$repo_name\" && git push -u origin master\n",
        "\n",
        "sub_info = {\n",
        "    'student_id': student_id,\n",
        "    'student_name': student_name, \n",
        "    'repo_url': repo_url,\n",
        "    'asg_dir_contents': os.listdir(str(ASSIGNMENT_PATH)),\n",
        "    'datetime': str(time.time()),\n",
        "    'asg_name': asg_name\n",
        "}\n",
        "json.dump(sub_info, open('info.json', 'w'))\n",
        "\n",
        "Javascript(script_save)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "file_id = drive.ListFile({'q':\"title='%s.ipynb'\"%asg_name}).GetList()[0]['id']\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('%s.ipynb'%asg_name) \n",
        "\n",
        "! jupyter nbconvert --to script \"$asg_name\".ipynb > /dev/null\n",
        "! jupyter nbconvert --to html \"$asg_name\".ipynb > /dev/null\n",
        "! zip \"$submission_file_name\" \"$asg_name\".ipynb \"$asg_name\".html \"$asg_name\".txt info.json > /dev/null\n",
        "\n",
        "print(\"##########################################\")\n",
        "print(\"Done! Submisson created, Please download using the bellow cell!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mX9OFzaLtYu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Submission (Run the cell)"
      ]
    },
    {
      "metadata": {
        "id": "PUzTlnX1nS8X",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "files.download(submission_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oJLRl0DL5aSO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# References\n"
      ]
    },
    {
      "metadata": {
        "id": "LCQPkvV83Kn0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Ioffe, Sergey, and Christian Szegedy. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” ArXiv:1502.03167 [Cs], February 10, 2015. http://arxiv.org/abs/1502.03167.\n",
        "* Im, Daniel Jiwoong, Michael Tao, and Kristin Branson. “An Empirical Analysis of the Optimization of Deep Network Loss Surfaces.” ArXiv:1612.04010 [Cs], December 12, 2016. http://arxiv.org/abs/1612.04010.\n",
        "* Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. “How Does Batch Normalization Help Optimization?” In Advances in Neural Information Processing Systems 31, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 2483–2493. Curran Associates, Inc., 2018. http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf.\n",
        "* Coursera Course: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\n",
        "* Intro to optimization in deep learning: Busting the myth about batch normalization [[link](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)]\n",
        "* Why Does Batch Normalization Work? [[link](https://abay.tech/blog/2018/07/01/why-does-batch-normalization-work/)]\n",
        "*  http://www.cs.toronto.edu/~tingwuwang/semantic_segmentation.pdf\n",
        "*  https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef\n",
        "* https://www.jeremyjordan.me/semantic-segmentation/\n",
        "\n"
      ]
    }
  ]
}